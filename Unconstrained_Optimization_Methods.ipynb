{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMnqtPNID3rKtK9XUubiFBz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pattichis/opt/blob/main/Unconstrained_Optimization_Methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Title\n"
      ],
      "metadata": {
        "id": "paOo_k83OiST"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "709VW5MtOkVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Library imports"
      ],
      "metadata": {
        "id": "Rrcg7sON5h6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import math\n",
        "import numpy as np\n",
        "import unittest\n",
        "\n",
        "from numpy import dot, transpose, array, diag, linalg, sqrt, identity, zeros, sign, arange\n",
        "\n",
        "from scipy.stats import uniform\n",
        "\n",
        "# alltrue is deprecated, use np.all() instead where needed\n",
        "def alltrue(A):\n",
        "  return np.all(A)\n",
        "\n",
        "# matrixmultiply() is deprecated\n",
        "def matrixmultiply(A, B):\n",
        "  return np.dot(A, B) # Performs dot product or matrix multiplication\n"
      ],
      "metadata": {
        "id": "HgkxL2X75h6u"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SuF0b7L7XumO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def MatQuad(l, M, r):\n",
        "\t\"\"\"\n",
        "\tIn several instances the product $l^T M r$ appears\n",
        "\tand so we define it here to simplify the\n",
        "\timplementation of various procedures.\n",
        "\n",
        "\tInputs:\n",
        "\t\tl is an n-dimensional vector\n",
        "\t\tM is an nxn matrix\n",
        "\t\tr is an n-dimensional vector\n",
        "\n",
        "\tOutputs:\n",
        "\t\treturns the product $l^TMr$\n",
        "\t\"\"\"\n",
        "\treturn matrixmultiply(matrixmultiply(transpose(l), M), r)"
      ],
      "metadata": {
        "id": "hioNXqQ9XvTK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Objective Functions"
      ],
      "metadata": {
        "id": "KqcEmz854eGY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Base class for objective functions"
      ],
      "metadata": {
        "id": "ZXVQZoDg4eGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ObjectiveFunction:\n",
        "\t\"\"\"\n",
        "\tThis is the base class for the objective functions that we\n",
        "\twill evaluate for optimization.  This class ensures that a function,\n",
        "\tits exact gradient and its exact Hessian are grouped into a single,\n",
        "\tlogical unit.\n",
        "\n",
        "\tFurthermore, this base class tracks the number of unique function\n",
        "\tevaluations required on a given optimization run.\n",
        "\n",
        "\tIt does this by storing the supplied inputs $x$ to each function\n",
        "\tin an array.  If the input $x$ is already in the array, it is\n",
        "\tnot added again.  Therefore, the number of unique function\n",
        "\tevaluations is the size of the array.\n",
        "\t\"\"\"\n",
        "\tdef __init__(self, ninit = 0):\n",
        "\t\tself.f_cache = []\n",
        "\t\tself.f_grad_cache = []\n",
        "\t\tself.f_hessian_cache = []\n",
        "\t\tself.n = ninit\n",
        "\n",
        "\tdef __check_n(self, x):\n",
        "\t\t\"\"\"\n",
        "\t\tThe first time an x is supplied to either real_f, grad_f,\n",
        "\t\tor hessian_f, we extract the dimension of this x and set\n",
        "\t\tour internal n value to it.\n",
        "\n",
        "\t\tOr, for functions of fixed dimesions, they may set n\n",
        "\t\tin their constructor.\n",
        "\n",
        "\n",
        "\t\tThis function either sets the n value if it hasn't been set,\n",
        "\t\tor compares the dimension of the x supplied to our n\n",
        "\t\tvalue to make sure that an x of appropriate dimension\n",
        "\t\thas been supplied.\n",
        "\t\t\"\"\"\n",
        "\t\tif self.n:\n",
        "\t\t\tif x.shape[0] != self.n:\n",
        "\t\t\t\traise 'bad dimension on x'\n",
        "\t\telse:\n",
        "\t\t\tself.n = x.shape[0]\n",
        "\n",
        "\n",
        "\tdef descriptive_name(self):\n",
        "\t\t\"\"\"\n",
        "\t\tReturn a descriptive name of the objective function, for\n",
        "\t\tuse in the table headers.\n",
        "\t\t\"\"\"\n",
        "\t\treturn 'generic objective function'\n",
        "\n",
        "\tdef eval_count(self):\n",
        "\t\t\"\"\"\n",
        "\t\tReturns the number of times f, grad_f, and hessian_f have\n",
        "\t\tbeen called with unique inputs.\n",
        "\t\t\"\"\"\n",
        "\t\treturn len(self.f_cache) + len(self.f_grad_cache) + \\\n",
        "\t\t\tlen(self.f_hessian_cache)\n",
        "\n",
        "\tdef real_f(self, x):\n",
        "\t\t\"\"\"\n",
        "\t\tThe actual function you want to override in a subclass.\n",
        "\n",
        "\t\tShould return the value of the function evaluated at $x$.\n",
        "\t\t\"\"\"\n",
        "\t\treturn 0\n",
        "\n",
        "\tdef f(self, x):\n",
        "\t\t\"\"\"\n",
        "\t\tReturns the value of the function evaluated at $x$.\n",
        "\n",
        "\t\tUses caching to count the number of unique function\n",
        "\t\tevaluations.\n",
        "\n",
        "\t\tThis is the function that the optimization methods\n",
        "\t\tshould call.\n",
        "\t\t\"\"\"\n",
        "\t\tself.__check_n(x)\n",
        "\n",
        "\t\tfor cache_x in self.f_cache:\n",
        "\t\t\t# x == cache_x on array types returns a matrix\n",
        "\t\t\t# of value-by-value comparisons.  The alltrue\n",
        "\t\t\t# numpy function returns true iff all comparisons\n",
        "\t\t\t# returned true\n",
        "\t\t\tif alltrue(cache_x == x):\n",
        "\t\t\t\treturn self.real_f(x)\n",
        "\n",
        "\t\tself.f_cache.append(x)\n",
        "\n",
        "\t\treturn self.real_f(x)\n",
        "\n",
        "\tdef real_grad_f(self, x):\n",
        "\t\t\"\"\"\n",
        "\t\tThe actual functino you want to override in a subclass.\n",
        "\n",
        "\t\tShould return the value of the exact gradient of the\n",
        "\t\tfunction $f$ evaluated at $x$.\n",
        "\t\t\"\"\"\n",
        "\t\treturn 0\n",
        "\n",
        "\tdef grad_f(self, x):\n",
        "\t\t\"\"\"\n",
        "\t\tReturns the value of the gradient of the function f\n",
        "\t\tevaluated at $x$.\n",
        "\t\t\"\"\"\n",
        "\t\tself.__check_n(x)\n",
        "\n",
        "\t\tfor cache_x in self.f_grad_cache:\n",
        "\t\t\tif alltrue(cache_x == x):\n",
        "\t\t\t\treturn self.real_grad_f(x)\n",
        "\n",
        "\t\tself.f_grad_cache.append(x)\n",
        "\n",
        "\t\treturn self.real_grad_f(x)\n",
        "\n",
        "\tdef real_hessian_f(self, x):\n",
        "\t\t\"\"\"\n",
        "\t\tThe actual function you want to override in a subclass.\n",
        "\n",
        "\t\tReturns the value of the exact Hessian of the function\n",
        "\t\t$f$ evaluated at $x$.\n",
        "\t\t\"\"\"\n",
        "\t\treturn 0\n",
        "\n",
        "\tdef hessian_f(self, x):\n",
        "\t\t\"\"\"\n",
        "\t\tReturns the value of the Hessian of the function f\n",
        "\t\tevaluated at $x$.\n",
        "\t\t\"\"\"\n",
        "\t\tself.__check_n(x)\n",
        "\n",
        "\t\tfor cache_x in self.f_hessian_cache:\n",
        "\t\t\tif alltrue(cache_x == x):\n",
        "\t\t\t\treturn self.real_hessian_f(x)\n",
        "\n",
        "\t\tself.f_hessian_cache.append(x)\n",
        "\n",
        "\t\treturn self.real_hessian_f(x)\n"
      ],
      "metadata": {
        "id": "KC2AtGvh4eGY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rosenbrock"
      ],
      "metadata": {
        "id": "bT0YmsD04eGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Rosenbrock(ObjectiveFunction):\n",
        "\t\"\"\"\n",
        "\tThe Rosenbrock function is defined in Nocedal and Wright's\n",
        "\ttextbook Numerical Optimization, Formula 2.23, on page 30.\n",
        "\n",
        "\tThe formula has been rewritten slightly to account for the fact\n",
        "\tthat the input array has two elements at indicies 0 and 1, whereas\n",
        "\tthe formula defines two elements $x_1$ and $x_2$.  The reader should\n",
        "\tconsider code $x_0$ = book $x_1$ and code $x_1$ = book $x_2$.\n",
        "\tTherefore we write it as:\n",
        "\n",
        "\t$f(x) = 100(x_1 - x_0^2)^2 + (1 - x_0)^2$\n",
        "\t\"\"\"\n",
        "\tdef __init__(self):\n",
        "\t\tObjectiveFunction.__init__(self)\n",
        "\t\tself.n = 2\n",
        "\n",
        "\tdef descriptive_name(self):\n",
        "\t\t\"\"\"\n",
        "\t\tReturn a descriptive name of the objective function, for\n",
        "\t\tuse in the table headers.\n",
        "\t\t\"\"\"\n",
        "\t\treturn 'the Rosenbrock function'\n",
        "\n",
        "\tdef real_f(self, x):\n",
        "\t\t\"\"\"\n",
        "\t\tReturns the value of Rosenbrock function evaluated at $x$.\n",
        "\n",
        "\t\tInputs:\n",
        "\t\t\t$x$ is a 2D Numeric array of floating point numbers which\n",
        "\t\t\trepresent the inputs to the Rosenbrock function.\n",
        "\n",
        "\t\tOutputs:\n",
        "\t\t\tReturns a floating point scalar with the value of the\n",
        "\t\t\tRosenbrock function evaluated at $x$.\n",
        "\n",
        "\t\tSide Effects:\n",
        "\t\t\tNone.\n",
        "\t\t\"\"\"\n",
        "\t\treturn (100 * (x[1] - x[0]**2)**2) + (1 - x[0])**2\n",
        "\n",
        "\tdef real_grad_f(self, x):\n",
        "\t\t\"\"\"\n",
        "\t\tReturns the value of the gradient of the Rosenbrock function\n",
        "\t\tevaluated at $x$.\n",
        "\n",
        "\t\tThe gradient of the Rosenbrock function was calculated by hand\n",
        "\t\tto be:\n",
        "\n",
        "\t\t$\\nabla f(x) = \\left[ \\begin{array}{c}\n",
        "\t\t400x_0^3 - 400x_1x_0 + 2x_0 - 2 \\\\\n",
        "\t\t-200x_0^2 + 200x_1 \\end{array} \\right]$\n",
        "\n",
        "\t\tInputs:\n",
        "            \t\t$x$ is a 2D Numeric array of floating point numbers\n",
        "\t\t\t\twhich represent the inputs to the Rosenbrock\n",
        "\t\t\t\tgradient function\n",
        "\n",
        "\t\tOutputs:\n",
        "\t\t\tReturns a 2D Numeric array with the values of the\n",
        "\t\t\tgradient of the Rosenbrock function evaluated at $x$.\n",
        "\n",
        "\t\tSide Effects:\n",
        "\t\t\tNone.\n",
        "\t    \"\"\"\n",
        "\t\treturn array((400*x[0]**3 - 400*x[1]*x[0] + 2*x[0] - 2,\n",
        "\t\t\t\t\t  -200*x[0]**2 + 200*x[1]))\n",
        "\n",
        "\tdef real_hessian_f(self, x):\n",
        "\t\t\"\"\"\n",
        "\t\tReturns the value of the Hessian of the Rosenbrock function\n",
        "\t\tevaluated at $x$.\n",
        "\n",
        "\t\tThe Hessian of the Rosenbrock function was calculated by hand\n",
        "\t\tto be:\n",
        "\n",
        "\t\t$\\nabla^2 f(x) = \\left[ \\begin{array}{cc}\n",
        "\t\t1200x_0^2 - 400x_1 + 2  & -400x_0 \\\\\n",
        "\t\t-400x_0 & 200 \\end{array} \\right]$\n",
        "\n",
        "\t\tInputs:\n",
        "\t\t\t$x$ is a 2D Numeric array of the floating point numbers\n",
        "\t\t\twhich represent the inputs to the Rosenbrock Hessian\n",
        "\t\t\tfunction.\n",
        "\n",
        "\t\tOutputs:\n",
        "\t\t\tReturns a 2x2 Numeric matrix with the values of\n",
        "\t\t\tthe Hessian of the Rosenbrock function evaluated at $x$.\n",
        "\n",
        "\t\tSide Effects:\n",
        "\t\t\tNone.\n",
        "\t\t\"\"\"\n",
        "\t\treturn array(([1200*x[0]**2 - 400*x[1] + 2, -400*x[0]],\n",
        "\t\t\t\t\t  [-400*x[0], 200]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cc67bbe-edd8-49ac-8da3-74ff97990ce4",
        "id": "7W1vfepX4eGY"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:50: SyntaxWarning: invalid escape sequence '\\l'\n",
            "<>:77: SyntaxWarning: invalid escape sequence '\\l'\n",
            "<>:50: SyntaxWarning: invalid escape sequence '\\l'\n",
            "<>:77: SyntaxWarning: invalid escape sequence '\\l'\n",
            "/tmp/ipython-input-1940060607.py:50: SyntaxWarning: invalid escape sequence '\\l'\n",
            "  $\\nabla f(x) = \\left[ \\begin{array}{c}\n",
            "/tmp/ipython-input-1940060607.py:77: SyntaxWarning: invalid escape sequence '\\l'\n",
            "  $\\nabla^2 f(x) = \\left[ \\begin{array}{cc}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 4.1 and 4.3 functions"
      ],
      "metadata": {
        "id": "Py1wSxun4eGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Problem4_1(ObjectiveFunction):\n",
        "\t\"\"\"\n",
        "\tThis class is an implementation of the function defined in\n",
        "\tProblem 4.1 of Nocedal and Wright, on page 97.\n",
        "\t\"\"\"\n",
        "\tdef descriptive_name(self):\n",
        "\t\treturn 'the function defined in problem 4.1'\n",
        "\n",
        "\tdef real_f(self, x):\n",
        "\t\t\"\"\"\n",
        "\t\tReturns the value of the function evaluated at $x$.\n",
        "\n",
        "\t\tThe function is defined to be:\n",
        "\n",
        "\t\t$f(x) = 10(x_2 - x_1^2)^2 + (1 - x_1)^2$\n",
        "       \t\t\"\"\"\n",
        "\t\treturn 10*(x[1] - x[0]**2)**2 + (1 - x[0])**2\n",
        "\n",
        "\tdef grad_f(self, x):\n",
        "\t\t\"\"\"\n",
        "\t\tReturns the value of the gradient of the function,\n",
        "\t\t$\\nabla f$ evaluated at $x$.\n",
        "\n",
        "\t\tThe gradient is defined to be:\n",
        "\n",
        "\t\t$\\nabla f(x) = \\left[ \\begin{array}{c}\n",
        "\t\t  -40x_1x_2 + 40x_1 - 2 + 2x_1 \\\\ 20x_2 - 20x_1^2 \\end{array} \\right]$\n",
        "\t\t\"\"\"\n",
        "\t\treturn array((-40.0*x[0]*x[1] + 40*x[0] - 2 + 2*x[0],\n",
        "\t\t\t      20*x[1] - 20*x[0]))\n",
        "\n",
        "\tdef hessian_f(self, x):\n",
        "\t\t\"\"\"\n",
        "\t\tReturns the value of the Hessian of the function,\n",
        "\t\t$\\nabla^2 f$ evaluated at $x$.\n",
        "\n",
        "\t\tThe Hessian is defined to be:\n",
        "\n",
        "\t\t$\\nabla^2 f(x) = \\left[ \\begin{array}{cc}\n",
        "\t\t  -40x_2 + 42 & -40x_1 \\\\ -40x_1 & 20 \\end{array} \\right]$\n",
        "\t\t\"\"\"\n",
        "\t\treturn array(([-40*x[1]+42, -40*x[0]], [-40*x[0], 20]))\n",
        "\n",
        "\n",
        "class Problem4_3(ObjectiveFunction):\n",
        "\t\"\"\"\n",
        "\tThis class is an implementation of the function defined in\n",
        "\tProblem 4.3 of Nocedal and Wright, on page 97.\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef descriptive_name(self):\n",
        "\t\treturn 'the function defined in problem 4.3, n = %d' % self.n\n",
        "\n",
        "\tdef real_f(self, x):\n",
        "\t\t\"\"\"\n",
        "\t\tReturns the value of the function evaluated at x.  Note that the\n",
        "\t\tdimension on x is taken to be twice the value of the $n$ in the\n",
        "\t\tsummation, as that is the smallest size vector that could be\n",
        "\t\tprocessed in this summation.\n",
        "\n",
        "\t\tThe function is defined to be:\n",
        "\n",
        "\t\t$f(x) = \\sum_{i=1}^n \\left[(1-x_{2i-1})^2 + 10(x_{2i} - x_{2i-1}^2)^2 \\right]$\n",
        "\t\t\"\"\"\n",
        "\t\taccum = 0.0\n",
        "\n",
        "\t\tfor i in range(0, self.n//2):\n",
        "\t\t\taccum += (1 - x[2*i])**2 + 10*(x[2*i+1] - x[2*i]**2)**2\n",
        "\n",
        "\t\treturn accum\n",
        "\n",
        "\tdef real_grad_f(self, x):\n",
        "\t\t\"\"\"\n",
        "\t\tReturns the value of the gradient of the function evaluated at x.\n",
        "\t\tThe $i$th component of the gradient, for $i$ valid from $0$ to\n",
        "\t\t$2n -1$ was calculated by hand to be:\n",
        "\n",
        "\t\t$\\nabla f_i(x) = \\left\\{ \\begin{array}{rcl}\n",
        "\t\t\t-2 + 2x_i - 40x_ix_{i+1} + 40x_i^3 & \\mbox{for} & i \\mbox{ even} \\\\\n",
        "\t\t\t20x_i - 20x_{i-1}^2                & \\mbox{for} & i \\mbox{ odd} \\end{array} \\right.$\n",
        "\t\t\"\"\"\n",
        "\t\tgrad_transpose = []\n",
        "\n",
        "\t\tfor i in range(0, self.n):\n",
        "\t\t\tif not i % 2:\n",
        "\t\t\t\t# i even\n",
        "\t\t\t\tgrad_transpose.append(-2 + 2*x[i] \\\n",
        "\t\t\t\t  - 40*x[i+1]*x[i] + 40*x[i]**3)\n",
        "\t\t\telse:\n",
        "\t\t\t\t# i odd\n",
        "\t\t\t\tgrad_transpose.append(20*x[i] - 20*x[i-1]**2)\n",
        "\n",
        "\t\treturn transpose(array((grad_transpose)))\n",
        "\n",
        "\tdef real_hessian_f(self, x):\n",
        "\t\t\"\"\"\n",
        "\t\tReturns the hessian of the function evaluated at $x$.\n",
        "\n",
        "\t\tJust constructs the diagonal terms, then uses numpy's\n",
        "\t\tdiag() operator to construct an nxn matrix with hessian\n",
        "\t\tas the diagonal members of that matrix.\n",
        "\n",
        "\t\tThe $ij$ compmonent of the hessian was calculated by\n",
        "\t\thand to be:\n",
        "\n",
        "\t\t$\\nabla^2 f_{i,j}(x) = \\left\\{ \\begin{array}{rcll}\n",
        "\t\t\t2 - 40x_{i+1} + 120x_i^2 & \\mbox{for} & i=j & i \\mbox{ even} \\\\\n",
        "\t\t\t20 & \\mbox{for} & i=j & i \\mbox{ odd} \\\\\n",
        "\t\t\t0 & & & \\mbox{otherwise} \\end{array} \\right.$\n",
        "\t\t\"\"\"\n",
        "\t\thessian = []\n",
        "\n",
        "\t\tfor i in range(0, self.n):\n",
        "\t\t\tif not i % 2:\n",
        "\t\t\t\thessian.append(2.0 - 40*x[i+1] + 120*x[i]**2)\n",
        "\t\t\telse:\n",
        "\t\t\t\thessian.append(20.0)\n",
        "\n",
        "\t\treturn diag(array((hessian)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4af55b70-b5f8-444f-b5fe-d38662e2c24f",
        "id": "iItbOHgY4eGZ"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:26: SyntaxWarning: invalid escape sequence '\\l'\n",
            "<>:39: SyntaxWarning: invalid escape sequence '\\l'\n",
            "<>:63: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:78: SyntaxWarning: invalid escape sequence '\\l'\n",
            "<>:106: SyntaxWarning: invalid escape sequence '\\l'\n",
            "<>:26: SyntaxWarning: invalid escape sequence '\\l'\n",
            "<>:39: SyntaxWarning: invalid escape sequence '\\l'\n",
            "<>:63: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:78: SyntaxWarning: invalid escape sequence '\\l'\n",
            "<>:106: SyntaxWarning: invalid escape sequence '\\l'\n",
            "/tmp/ipython-input-1870734557.py:26: SyntaxWarning: invalid escape sequence '\\l'\n",
            "  $\\nabla f(x) = \\left[ \\begin{array}{c}\n",
            "/tmp/ipython-input-1870734557.py:39: SyntaxWarning: invalid escape sequence '\\l'\n",
            "  $\\nabla^2 f(x) = \\left[ \\begin{array}{cc}\n",
            "/tmp/ipython-input-1870734557.py:63: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  $f(x) = \\sum_{i=1}^n \\left[(1-x_{2i-1})^2 + 10(x_{2i} - x_{2i-1}^2)^2 \\right]$\n",
            "/tmp/ipython-input-1870734557.py:78: SyntaxWarning: invalid escape sequence '\\l'\n",
            "  $\\nabla f_i(x) = \\left\\{ \\begin{array}{rcl}\n",
            "/tmp/ipython-input-1870734557.py:106: SyntaxWarning: invalid escape sequence '\\l'\n",
            "  $\\nabla^2 f_{i,j}(x) = \\left\\{ \\begin{array}{rcll}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objective functions for verifying line search"
      ],
      "metadata": {
        "id": "cfgoxVqJ4eGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class testObjFunc(ObjectiveFunction):\n",
        "\t\"\"\"\n",
        "\tA simple test Objective Function, with root\n",
        "\tat $x^\\ast = (0,0)$, for testing the Line Search\n",
        "\tmethods.\n",
        "\n",
        "\t$f(x) = x_0^2 + x_1^2$\n",
        "\t\"\"\"\n",
        "\tdef descriptive_name(self):\n",
        "\t\treturn 'test function'\n",
        "\n",
        "\tdef real_f(self, x):\n",
        "\t\treturn x[0]**2 + x[1]**2\n",
        "\n",
        "\tdef real_grad_f(self, x):\n",
        "\t\t\"\"\"\n",
        "\t\tThe gradient $\\nabla f$ is calculated to be:\n",
        "\n",
        "\t\t$\\nabla f = \\left[ \\begin{array}{c}\n",
        "\t\t   2x_0 \\\\ 2x_1 \\end{array} \\right]$\n",
        "\t\t\"\"\"\n",
        "\t\treturn array((2.0*x[0], 2.0*x[1]))\n",
        "\n",
        "\tdef real_hessian_f(self, x):\n",
        "\t\t\"\"\"\n",
        "\t\tThe hessian $\\nabla^2 f$ is calculated to be:\n",
        "\n",
        "\t\t$\\nabla^2 f = \\left[ \\begin{array}{cc}\n",
        "\t\t  2 & 0 \\\\ 0 & 2 \\end{array} \\right]$\n",
        "\t\t\"\"\"\n",
        "\t\treturn array(([2.0, 0.0], [0.0, 2.0]))\n",
        "\n",
        "class testSecondFunc(ObjectiveFunction):\n",
        "\t\"\"\"\n",
        "\tA simple test objective function, with root at\n",
        "\t$x^\\ast = (1,1)$ for testing the line search methods.\n",
        "\tThis one is a bit more ill conditioned for a line search\n",
        "\tin that the variables do not contribute equally to\n",
        "\tthe solution of the problem.\n",
        "\n",
        "\t$f(x) = 250.0(x_0 - 1)^2 + 0.025*(x_1 - 1)^2$\n",
        "\t\"\"\"\n",
        "\tdef descriptive_name(self):\n",
        "\t\treturn 'a second test function'\n",
        "\n",
        "\tdef real_f(self, x):\n",
        "\t\t\"\"\"\n",
        "\t\tReturns the function evaluated at $x$\n",
        "\t\t\"\"\"\n",
        "\t\treturn 250.0*(x[0] - 1.0)**2 + 0.025*(x[1] - 1.0)**2\n",
        "\n",
        "\tdef real_grad_f(self, x):\n",
        "\t\t\"\"\"\n",
        "\t\tThe gradient $\\nabla f$ is calculated to be:\n",
        "\n",
        "\t\t$\\nabla f = \\left[ \\begin{array}{c}\n",
        "\t\t  500x_0 - 500 \\\\ 0.05x_1 - 0.05 \\end{array} \\right]$\n",
        "\t\t\"\"\"\n",
        "\t\treturn array((500*x[0] - 500, 0.05*x[1] - 0.05))\n",
        "\n",
        "\tdef real_hessian_f(self, x):\n",
        "\t\t\"\"\"\n",
        "\t\tThe hessian $\\nabla^2 f$ is calculated to be:\n",
        "\n",
        "\t\t$\\nabla^2 f = \\left[ \\begin{array}{cc}\n",
        "\t\t  500 & 0 \\\\ 0 & 0.05 \\end{array} \\right]$\n",
        "\t\t\"\"\"\n",
        "\t\treturn array(([500, 0.0], [0.0, 0.05]))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4816c538-1d06-4aa4-9f7d-dbff6b361419",
        "id": "5tUmW4LK4eGZ"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:19: SyntaxWarning: invalid escape sequence '\\l'\n",
            "<>:28: SyntaxWarning: invalid escape sequence '\\l'\n",
            "<>:56: SyntaxWarning: invalid escape sequence '\\l'\n",
            "<>:65: SyntaxWarning: invalid escape sequence '\\l'\n",
            "<>:19: SyntaxWarning: invalid escape sequence '\\l'\n",
            "<>:28: SyntaxWarning: invalid escape sequence '\\l'\n",
            "<>:56: SyntaxWarning: invalid escape sequence '\\l'\n",
            "<>:65: SyntaxWarning: invalid escape sequence '\\l'\n",
            "/tmp/ipython-input-1420064506.py:19: SyntaxWarning: invalid escape sequence '\\l'\n",
            "  $\\nabla f = \\left[ \\begin{array}{c}\n",
            "/tmp/ipython-input-1420064506.py:28: SyntaxWarning: invalid escape sequence '\\l'\n",
            "  $\\nabla^2 f = \\left[ \\begin{array}{cc}\n",
            "/tmp/ipython-input-1420064506.py:56: SyntaxWarning: invalid escape sequence '\\l'\n",
            "  $\\nabla f = \\left[ \\begin{array}{c}\n",
            "/tmp/ipython-input-1420064506.py:65: SyntaxWarning: invalid escape sequence '\\l'\n",
            "  $\\nabla^2 f = \\left[ \\begin{array}{cc}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit tests for objective functions"
      ],
      "metadata": {
        "id": "qJeDT-OH4eGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#####################\n",
        "# unit testing code #\n",
        "#####################\n",
        "import unittest\n",
        "import numpy as np # Add numpy import explicitly\n",
        "\n",
        "class ObjectiveFunctionTestCase(unittest.TestCase):\n",
        "        def testFunctionCaching(self):\n",
        "                \"\"\"\n",
        "                Check that the ObjectiveFunction class correctly\n",
        "                counts the number of unique inputs to f\n",
        "                \"\"\"\n",
        "                o = ObjectiveFunction()\n",
        "\n",
        "                assert len(o.f_cache) == 0, \\\n",
        "                        'incorrect starting count for unique inputs'\n",
        "                o.f(np.array((0.0, 0.0)))\n",
        "                assert len(o.f_cache) == 1, \\\n",
        "                        'incorrect count for one input'\n",
        "                o.f(np.array((0.0, 0.0)))\n",
        "                assert len(o.f_cache) == 1, \\\n",
        "                        'incorrect count for one unique input'\n",
        "                o.f(np.array((1.0, 0.0)))\n",
        "                assert len(o.f_cache) == 2, \\\n",
        "                        'incorrect count for two unique inputs'\n",
        "\n",
        "                # check that the f cache didn't end up corrupting\n",
        "                # the other caches\n",
        "                assert len(o.f_grad_cache) == 0, \\\n",
        "                        'f cache corrupted gradient cache'\n",
        "                assert len(o.f_hessian_cache) == 0, \\\n",
        "                        'f cache corrupted hessian cache'\n",
        "\n",
        "                # check gradient cache\n",
        "                o.grad_f(np.array((1.0, 0.0)))\n",
        "                assert len(o.f_grad_cache) == 1, \\\n",
        "                        'f grad cache not storing inputs'\n",
        "                o.grad_f(np.array((1.0, 0.0)))\n",
        "                assert len(o.f_grad_cache) == 1, \\\n",
        "                        'f grad cache not storing unique inputs'\n",
        "\n",
        "                # check Hessian cache\n",
        "                o.hessian_f(np.array((0.0, 1.0)))\n",
        "                assert len(o.f_hessian_cache) == 1, \\\n",
        "                        'f hessian cache not storing inputs'\n",
        "                o.hessian_f(np.array((0.0, 1.0)))\n",
        "                assert len(o.f_hessian_cache) == 1, \\\n",
        "                        'f hessian cache not storing unique inputs'\n",
        "\n",
        "class RosenbrockTestCase(unittest.TestCase):\n",
        "        def testRosenbrock(self):\n",
        "                \"\"\"\n",
        "                Check that the Rosenbrock function returns correct\n",
        "                output for certain inputs\n",
        "                \"\"\"\n",
        "                r = Rosenbrock()\n",
        "\n",
        "                assert r.f(np.array((0.0, 0.0))) == 1, \\\n",
        "                        'incorrect value returned for x=[0,0]'\n",
        "                assert r.f(np.array((1.0, 0.0))) == 100, \\\n",
        "                        'incorrect value returned for x=[1,0]'\n",
        "                assert r.f(np.array((0.0, 1.0))) == 101, \\\n",
        "                        'incorrect value returned for x=[0,1]'\n",
        "                # check the case of the minimum of the Rosenbrock at\n",
        "                # $x^\\ast = (1,1)$\n",
        "                assert r.f(np.array((1.0, 1.0))) == 0, \\\n",
        "                        'incorrect value returned for x=[1,1]'\n",
        "\n",
        "\n",
        "        def testRosenbrockGradient(self):\n",
        "                \"\"\"\n",
        "                Check that the Rosenbrock gradient function\n",
        "                returns correct output for certain inputs\n",
        "                \"\"\"\n",
        "                r = Rosenbrock()\n",
        "\n",
        "                assert np.all(r.grad_f(np.array((0.0, 0.0)))\\\n",
        "                               == np.array((-2.0, 0.0))), \\\n",
        "                        'incorrect value returned for x=[0,0]'\n",
        "                assert np.all(r.grad_f(np.array((1.0, 0.0))) \\\n",
        "                               == np.array((400.0, -200.0))), \\\n",
        "                        'incorrect value returned for x=[1,0]'\n",
        "                assert np.all(r.grad_f(np.array((0.0, 1.0))) \\\n",
        "                               == np.array((-2.0, 200.0))), \\\n",
        "                        'incorrect value returned for x=[0,1]'\n",
        "                # gradient should be zero at the minimum of the Rosenbrock at\n",
        "                # $x^\\ast = (1,1)$\n",
        "                assert np.all(r.grad_f(np.array((1.0, 1.0))) == \\\n",
        "                               np.array((0.0, 0.0))), \\\n",
        "                        'incorrect value returned for x=[1,1]'\n",
        "\n",
        "        def testRosenbrockHessian(self):\n",
        "                \"\"\"\n",
        "                Check that the Rosenbrock Hessian function\n",
        "                returns correct output for certain inputs\n",
        "                \"\"\"\n",
        "                r = Rosenbrock()\n",
        "                assert np.all(r.hessian_f(np.array((0.0, 0.0))) \\\n",
        "                               == np.array(([2, 0],\\\n",
        "                        [0, 200]))), 'incorrect value returned for x=[0,0]'\n",
        "                assert np.all(r.hessian_f(np.array((1.0, 0.0))) \\\n",
        "                               == np.array(([1202, -400],\\\n",
        "                        [-400, 200]))), 'incorrect value returned for x=[1,0]'\n",
        "                assert np.all(r.hessian_f(np.array((0.0, 1.0))) \\\n",
        "                               == np.array(([-398, 0],\\\n",
        "                        [0, 200]))), 'incorrect value returned for x=[0,1]'\n",
        "                # Hessian should be pos def at the minimum of the Rosenbrock\n",
        "                # at $x^\\ast= (1,1)$\n",
        "                assert np.all(r.hessian_f(np.array((1.0, 1.0))) \\\n",
        "                               == np.array(([802, -400],\\\n",
        "                        [-400, 200]))), 'incorrect value returned for x=[1,1]'\n",
        "\n",
        "class Problem4_1TestCase(unittest.TestCase):\n",
        "        def testProblem4_1(self):\n",
        "                \"\"\"\n",
        "                Check that the Problem4_1 function returns correct\n",
        "                output for certain inputs\n",
        "                \"\"\"\n",
        "                p = Problem4_1()\n",
        "\n",
        "                print(p.f(np.array((0.0, 0.0))))\n",
        "\n",
        "                assert p.f(np.array((0.0, 0.0))) == 1, \\\n",
        "                        'incorrect value returned for x=[0,0]'\n",
        "                assert p.f(np.array((1.0, 0.0))) == 10, \\\n",
        "                        'incorrect value returned for x=[1,0]'\n",
        "                assert p.f(np.array((0.0, 1.0))) == 11, \\\n",
        "                        'incorrect value returned for x=[0,1]'\n",
        "                # check the case of the minimum of Problem4_1 at\n",
        "                # $x^\\ast = (1,1)$\n",
        "                assert p.f(np.array((1.0, 1.0))) == 0, \\\n",
        "                        'incorrect value returned for x=[1,1]'\n",
        "\n",
        "\n",
        "        def testProblem4_1Gradient(self):\n",
        "                \"\"\"\n",
        "                Check that the Problem4_1 gradient function\n",
        "                returns correct output for certain inputs\n",
        "                \"\"\"\n",
        "                p = Problem4_1()\n",
        "\n",
        "                assert np.all(p.grad_f(np.array((0.0, 0.0))) == np.array((-2.0, 0.0))), \\\n",
        "                        'incorrect value returned for x=[0,0]'\n",
        "                assert np.all(p.grad_f(np.array((1.0, 0.0))) == np.array((40.0, -20.0))), \\\n",
        "                        'incorrect value returned for x=[1,0]'\n",
        "                assert np.all(p.grad_f(np.array((0.0, 1.0))) == np.array((-2.0, 20.0))), \\\n",
        "                        'incorrect value returned for x=[0,1]'\n",
        "                # gradient should be zero at the minimum of the Problem4_1 at\n",
        "                # $x^\\ast = (1,1)$\n",
        "                assert np.all(p.grad_f(np.array((1.0, 1.0))) == np.array((0.0, 0.0))), \\\n",
        "                        'incorrect value returned for x=[1,1]'\n",
        "\n",
        "        def testProblem4_1Hessian(self):\n",
        "                \"\"\"\n",
        "                Check that the Problem4_1 Hessian function\n",
        "                returns correct output for certain inputs\n",
        "                \"\"\"\n",
        "                p = Problem4_1()\n",
        "                print(\"\\n--- Debugging testProblem4_1Hessian ---\")\n",
        "\n",
        "                x_test = np.array((0.0, 0.0))\n",
        "                calculated_hessian = p.hessian_f(x_test)\n",
        "                expected_hessian = np.array(([42, 0],[0, 20]))\n",
        "                print(f\"Input x: {x_test}\")\n",
        "                print(f\"Calculated Hessian: {calculated_hessian}\")\n",
        "                print(f\"Expected Hessian: {expected_hessian}\")\n",
        "                assert np.all(calculated_hessian == expected_hessian), f\"Calculated Hessian: {calculated_hessian} at x=[0, 0]\"\n",
        "\n",
        "                x_test = np.array((1.0, 0.0))\n",
        "                calculated_hessian = p.hessian_f(x_test)\n",
        "                expected_hessian = np.array(([42, -40],[-40, 20]))\n",
        "                print(f\"Input x: {x_test}\")\n",
        "                print(f\"Calculated Hessian: {calculated_hessian}\")\n",
        "                print(f\"Expected Hessian: {expected_hessian}\")\n",
        "                assert np.all(calculated_hessian == expected_hessian), 'incorrect value returned for x=[1,0]'\n",
        "\n",
        "                x_test = np.array((0.0, 1.0))\n",
        "                calculated_hessian = p.hessian_f(x_test)\n",
        "                expected_hessian = np.array(([2, 0],[0, 20]))\n",
        "                print(f\"Input x: {x_test}\")\n",
        "                print(f\"Calculated Hessian: {calculated_hessian}\")\n",
        "                print(f\"Expected Hessian: {expected_hessian}\")\n",
        "                assert np.all(calculated_hessian == expected_hessian), 'incorrect value returned for x=[0,1]'\n",
        "\n",
        "                # Hessian should be pos def at the minimum of the Problem4_1\n",
        "                # at $x^\\ast= (1,1)$\n",
        "                x_test = np.array((1.0, 1.0))\n",
        "                calculated_hessian = p.hessian_f(x_test)\n",
        "                expected_hessian = np.array(([2, -40],[-40, 20]))\n",
        "                print(f\"Input x: {x_test}\")\n",
        "                print(f\"Calculated Hessian: {calculated_hessian}\")\n",
        "                print(f\"Expected Hessian: {expected_hessian}\")\n",
        "                assert np.all(calculated_hessian == expected_hessian), 'incorrect value returned for x=[1,1]'\n",
        "\n",
        "\n",
        "class Problem4_3TestCase(unittest.TestCase):\n",
        "        def testFunctionValues(self):\n",
        "                pf = Problem4_3()\n",
        "\n",
        "                assert pf.f(np.array((0.0, 0.0))) == 1, \\\n",
        "                        'incorrect value returned for x=[0,0]'\n",
        "                assert pf.f(np.array((1.0, 0.0))) == 10, \\\n",
        "                        'incorrect value returned for x=[1,0]'\n",
        "                assert pf.f(np.array((0.0, 1.0))) == 11, \\\n",
        "                        'incorrect value returned for x=[0,1]'\n",
        "                assert pf.f(np.array((1.0, 1.0))) == 0, \\\n",
        "                        'incorrect value returned for x=[1,1]'\n",
        "\n",
        "        def testFunctionGradientValues(self):\n",
        "                pf = Problem4_3()\n",
        "\n",
        "                assert np.all(pf.grad_f(np.array((0.0, 0.0))) == np.array((-2.0, 0.0))), \\\n",
        "                        'incorrect value returned for x=[0,0]'\n",
        "                assert np.all(pf.grad_f(np.array((1.0, 0.0))) == np.array((40.0, -20.0))), \\\n",
        "                        'incorrect value returned for x=[1,0]'\n",
        "                assert np.all(pf.grad_f(np.array((0.0, 1.0))) == np.array((-2.0, 20.0))), \\\n",
        "                        'incorrect value returned for x=[0,1]'\n",
        "                assert np.all(pf.grad_f(np.array((1.0, 1.0))) == np.array((0.0, 0.0))), \\\n",
        "                        'incorrect value returned for x=[1,1]'\n",
        "\n",
        "        def testFunctionHessianValues(self):\n",
        "                pf = Problem4_3()\n",
        "\n",
        "                assert np.all(pf.hessian_f(np.array((0.0, 0.0))) == np.array(([2.0, 0.0],\\\n",
        "                                                                         [0.0, 20.0]))), \\\n",
        "                        'incorrect value returned for x=[0,0]'\n",
        "                assert np.all(pf.hessian_f(np.array((1.0, 0.0))) == np.array(([122.0, 0.0], \\\n",
        "                                                                        [0.0, 20.0]))), \\\n",
        "                         'incorrect value returned for x=[1,0]'\n",
        "                assert np.all(pf.hessian_f(np.array((0.0, 1.0))) == np.array(([-38.0, 0.0], \\\n",
        "                                                                         [0.0, 20]))), \\\n",
        "                         f\"incorrect value returned for x=[0,1] {pf.hessian_f(np.array((0.0, 1.0)))}\"\n",
        "\n",
        "                assert np.all(pf.hessian_f(np.array((1.0, 1.0))) == np.array(([82.0, 0.0], \\\n",
        "                                                                        [0.0, 20.0]))), \\\n",
        "                         f\"incorrect value returned for x=[1,1] {pf.hessian_f(np.array((1.0, 1.0)))}\"\n",
        "\n",
        "                pf4 = Problem4_3()\n",
        "                assert np.all(pf4.hessian_f(np.array((1.0, 1.0, 1.0, 1.0))) == \\\n",
        "                        np.array(([82.0, 0.0,  0.0,  0.0],\n",
        "                                [0.0, 20.0,  0.0,  0.0],\n",
        "                                [0.0,  0.0, 82.0,  0.0],\n",
        "                                [0.0,  0.0,  0.0, 20.0]))), \\\n",
        "                         'incorrect value returned for x=[1,1,1,1]'"
      ],
      "metadata": {
        "id": "-IME1YnO4eGZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18ecf26d-ee21-4129-b0d1-4195d4869368",
        "id": "qvYDlKR84eGZ"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "..........\n",
            "----------------------------------------------------------------------\n",
            "Ran 10 tests in 0.010s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "\n",
            "--- Debugging testProblem4_1Hessian ---\n",
            "Input x: [0. 0.]\n",
            "Calculated Hessian: [[42. -0.]\n",
            " [-0. 20.]]\n",
            "Expected Hessian: [[42  0]\n",
            " [ 0 20]]\n",
            "Input x: [1. 0.]\n",
            "Calculated Hessian: [[ 42. -40.]\n",
            " [-40.  20.]]\n",
            "Expected Hessian: [[ 42 -40]\n",
            " [-40  20]]\n",
            "Input x: [0. 1.]\n",
            "Calculated Hessian: [[ 2. -0.]\n",
            " [-0. 20.]]\n",
            "Expected Hessian: [[ 2  0]\n",
            " [ 0 20]]\n",
            "Input x: [1. 1.]\n",
            "Calculated Hessian: [[  2. -40.]\n",
            " [-40.  20.]]\n",
            "Expected Hessian: [[  2 -40]\n",
            " [-40  20]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7f36b5357560>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Line Search Algorithms"
      ],
      "metadata": {
        "id": "BGpQrGFsxyAM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BacktrackingAlpha(.): Find alpha to satisfy linear approximation"
      ],
      "metadata": {
        "id": "k6eDWgMGx7yX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def BacktrackingAlpha(fobj, x_k, p_k, alpha_bar):\n",
        "\t\"\"\"\n",
        "\tUses the backtracking procedure defined on page 41\n",
        "\tof Nocedal and Wright to find the $\\alpha_k$ to\n",
        "\tbe used for an iteration of a line search\n",
        "\talgorithm.\n",
        "\n",
        "\tInputs:\n",
        "\t\tfobj is a function object from ObjectiveFunctions.py\n",
        "\n",
        "\t\tx_k is the $x_k$ point that we are finding $\\alpha$ in\n",
        "\n",
        "\t\tp_k is the $p_k$ search direction\n",
        "\n",
        "\t\talpha_bar is the $\\overline{\\alpha}$, the initial value for $\\alpha$\n",
        "\n",
        "\tOutputs:\n",
        "\t\tThe $\\alpha$ value to use for this iteration\n",
        "\t\"\"\"\n",
        "\t# $\\rho$ and $c$ are constants that we set here\n",
        "\trho = 0.5\n",
        "\tc = 1e-4\n",
        "\n",
        "\t# $\\alpha \\leftarrow \\overline{\\alpha}$\n",
        "\talpha = alpha_bar\n",
        "\t# repeat until\n",
        "\t# $f(x_k + \\alpha p_k) \\leq f(x_k) + c \\alpha \\nabla f_k^T p_k$\n",
        "\twhile fobj.f(x_k + alpha * p_k) > fobj.f(x_k) + \\\n",
        "\t\t\tc * alpha * matrixmultiply(\\\n",
        "\t\ttranspose(fobj.grad_f(x_k)), p_k):\n",
        "\t\t# $\\alpha \\leftarrow \\rho \\alpha$\n",
        "\t\talpha = rho * alpha\n",
        "\n",
        "\treturn alpha\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ao693XEex187",
        "outputId": "3560f37e-1f73-4ef7-fbe3-9ded690f3275"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:15: SyntaxWarning: invalid escape sequence '\\o'\n",
            "<>:15: SyntaxWarning: invalid escape sequence '\\o'\n",
            "/tmp/ipython-input-2396795012.py:15: SyntaxWarning: invalid escape sequence '\\o'\n",
            "  alpha_bar is the $\\overline{\\alpha}$, the initial value for $\\alpha$\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Newtons Method Line Search"
      ],
      "metadata": {
        "id": "Ejn2y0ZCyK9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def NewtonsMethodLineSearch(x_0, alpha_0, f_alpha_k, fobj, xtol):\n",
        "\t\"\"\"\n",
        "\tFinds a point within a small distance from a stationary point\n",
        "\tof the function f using the basic line search algorithm defined\n",
        "\tin chapter 3 of Nocedal and Wright's Numerical Optimization, where\n",
        "\n",
        "\t$x_{k+1} = x_k + \\alpha_k p_k$\n",
        "\n",
        "\tFor Newton's Method, the $B_k = \\nabla^2f(x_k)$, so\n",
        "\n",
        "\t$p_k = -\\nabla^2f(x_k)\\nabla f(x_k)$\n",
        "\n",
        "\tThis function supports arbitrary line search procedures\n",
        "\tto find the $\\alpha_k$.\n",
        "\n",
        "\tInputs:\n",
        "\t\tx_0 is a Numeric array giving the value of $x_0$.\n",
        "\n",
        "\t\talpha_0 is a floating point scalar giving the value of $\\alpha_0$.\n",
        "\n",
        "\t\tf_alpha_k is a function reference, a function that takes an\n",
        "\t\t\t$x_k$, a $p_k$, and an $\\overline{\\alpha}$ and returns\n",
        "\t\t\tan $alpha_k$.  For reference see BacktrackingAlpha above.\n",
        "\n",
        "\t\tfobj is an ObjectiveFunction object.  The functions must take\n",
        "\t\t\tinputs of the same degree as x_0.\n",
        "\n",
        "\t\txtol is the tolerance on the stationary point.  The\n",
        "\t\t\talgorithm will terminate when $\\|\\nabla f(x_k)\\| \\leq$ xtol.\n",
        "\n",
        "\tOutputs:\n",
        "\t\tReturns the computed value of the stationary point of $f$, $x^\\ast$.\n",
        "\n",
        "\tSide Effects:\n",
        "\t\tWrites the values of $x_k$, $\\alpha_k$, and $p_k$ at each iteration\n",
        "\t\tto stdout.\n",
        "\t\"\"\"\n",
        "\tx_k = x_0\n",
        "\titerations = 0\n",
        "\n",
        "\t# print out our table header\n",
        "\tprint ('\\n\\nNewton\\'s Method Line Search on %s' \\\n",
        "\t\t% fobj.descriptive_name())\n",
        "\tprint ('alpha_0 = %f, x_0 = %s' % (alpha_0, x_0))\n",
        "\tprint('{:10s}{:10s}{:10s}{:10s}'.format('iteration', 'alpha_k', 'x_k', 'f evals'))\n",
        "\n",
        "\t# repeat loop until $\\|\\nabla f(x_k)\\| \\leq$ xtol\n",
        "\twhile dot(fobj.grad_f(x_k), fobj.grad_f(x_k)) > xtol:\n",
        "\t\t# $p_k = -B_k^{-1}\\nabla f_k = -\\nabla^2 f(x_k) \\nabla f_k$\n",
        "\t\tp_k = -1.0*matrixmultiply(linalg.inv(fobj.hessian_f(x_k)), \\\n",
        "\t\t\t\t\t\t\t  fobj.grad_f(x_k))\n",
        "\n",
        "    # apply the line search technique supplied\n",
        "\t\talpha_k = f_alpha_k(fobj, x_k, p_k, alpha_0)\n",
        "\n",
        "    # $x_{k+1} = x_k + \\alpha_k p_k$\n",
        "\t\tx_k = x_k + (alpha_k * p_k)\n",
        "\t\titerations += 1\n",
        "\n",
        "\t\t# print out the line of the table\n",
        "\t\tprint(f\"{iterations:10d}{alpha_k:10.4f}{x_k}{fobj.eval_count():10d}\")\n",
        "\n",
        "\t\t# make sure that this line gets written to stdout\n",
        "\t\tsys.stdout.flush()\n",
        "\n",
        "\treturn x_k\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyvB5YZSyYiD",
        "outputId": "2253eec1-9f68-4902-8556-a87acfaa288f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:22: SyntaxWarning: invalid escape sequence '\\o'\n",
            "<>:22: SyntaxWarning: invalid escape sequence '\\o'\n",
            "/tmp/ipython-input-3677109062.py:22: SyntaxWarning: invalid escape sequence '\\o'\n",
            "  $x_k$, a $p_k$, and an $\\overline{\\alpha}$ and returns\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Steepest Descent Line Search"
      ],
      "metadata": {
        "id": "CNFHWspvz7fB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SteepestDescentLineSearch(x_0, alpha_0, f_alpha_k, fobj, xtol):\n",
        "\t\"\"\"\n",
        "\tUses the steepest decent line search, in which $B_k = I$,\n",
        "\tto find the minimum value of the objective function.\n",
        "\n",
        "\tInputs:\n",
        "\t\tx_0 is the Numeric array giving the value of $x_0$\n",
        "\n",
        "\t\talpha_0 is a floating point scalar giving the value of $\\alpha_0$\n",
        "\n",
        "\t\tf_alpha_k is a function reference, a function that takes an\n",
        "\t\t\t$x_k$, a $p_k$, and an $\\overline{\\alpha}$ and returns\n",
        "\t\t\tan $alpha_k$.  For reference see BacktrackingAlpha above.\n",
        "\n",
        "\n",
        "\t\tfobj is an ObjectiveFunction object.  The functions must take\n",
        "\t\t\tinputs of the same degree as x_0.\n",
        "\n",
        "\t\txtol is the tolerance on the stationary point.  The\n",
        "\t\t\talgorithm will terminate when $\\|\\nabla f(x_k)\\|^2 \\leq$ xtol.\n",
        "\n",
        "\tOutputs:\n",
        "\t\tReturns the computed value of the stationary point of $f$, $x^\\ast$\n",
        "\t\twithin tolerance xtol.\n",
        "\n",
        "\tSide Effects:\n",
        "\t\tWrites the values of $x_k$, $\\alpha_k$, and $p_k$ at each iteration\n",
        "\t\tto stdout.\n",
        "\t\"\"\"\n",
        "\tx_k = x_0\n",
        "\titerations = 0\n",
        "\n",
        "\t# print out our table header\n",
        "\tprint ('\\n\\nSteepest Descent Line Search on %s' \\\n",
        "\t\t% fobj.descriptive_name())\n",
        "\tprint ('alpha_0 = %f, x_0 = %s' % (alpha_0, x_0))\n",
        "\tprint ('iteration\\talpha_k\\t\\tx_k\\t\\t\\t\\tf evals')\n",
        "\n",
        "\t# repeat loop until $\\|\\nabla f(x_k)\\| \\leq$ xtol\n",
        "\twhile dot(fobj.grad_f(x_k), fobj.grad_f(x_k)) > xtol:\n",
        "\t\t# $p_k = -B_k^{-1}\\nabla f_k = -\\nabla f_k$\n",
        "\t\tp_k = -1.0 * fobj.grad_f(x_k)\n",
        "\n",
        "\t\t# apply the line search technique supplied\n",
        "\t\talpha_k = f_alpha_k(fobj, x_k, p_k, alpha_0)\n",
        "\n",
        "\t\t# $x_{k+1} = x_k + \\alpha_k p_k$\n",
        "\t\tx_k = x_k + (alpha_k * p_k)\n",
        "\t\titerations += 1\n",
        "\n",
        "\t\t# print out a line of the table\n",
        "\t\tprint ('%d\\t\\t%f\\t%s\\t%d' \\\n",
        "\t\t\t% (iterations, alpha_k, x_k, fobj.eval_count()))\n",
        "\n",
        "\t\t# make sure that this line gets written to stdout\n",
        "\t\tsys.stdout.flush()\n",
        "\n",
        "\treturn x_k\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dl_lOCtsz75J",
        "outputId": "4c66a840-b2c2-49f7-d42a-227986306a91"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:12: SyntaxWarning: invalid escape sequence '\\o'\n",
            "<>:12: SyntaxWarning: invalid escape sequence '\\o'\n",
            "/tmp/ipython-input-2888884521.py:12: SyntaxWarning: invalid escape sequence '\\o'\n",
            "  $x_k$, a $p_k$, and an $\\overline{\\alpha}$ and returns\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## zoom(.) Strong Wolfe Conditions"
      ],
      "metadata": {
        "id": "GwCI-Dd917fS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def zoom(fobj, x_k, p_k, alpha_lo, alpha_hi, phi_0, phi_prime_0, c_1, c_2):\n",
        "\t\"\"\"\n",
        "\tThis is an implementation of Algorithm 3.3, page 60 of Nocedal and Wrigt\n",
        "\twhich is a subprocedure to implement the Strong Wolfe Conditions line\n",
        "\tsearch provided below in the Strong Wolfe function.\n",
        "\n",
        "\tInputs:\n",
        "\t\tfobj is an objective function object\n",
        "\n",
        "\t\tx_k is the $x_k$ value, a Numeric array, used to find $\\alpha$\n",
        "\n",
        "\t\tp_k is the $p_k$ value, a Numeric array, used to find $\\alpha$\n",
        "\n",
        "\t\talpha_lo is the $\\alpha_{lo}$ value, a floating point scalar\n",
        "\n",
        "\t\talpha_hi is the $\\alpha_{hi}$ value, a floating point scalar\n",
        "\n",
        "\t\tphi_0 is $\\phi(0)$, a floating point scalar\n",
        "\n",
        "\t\tphi_prime_0 is $\\phi'(0)$, a floating point scalar\n",
        "\n",
        "\t\tc_1 is the constant $c_1$ from the Strong Wolfe conditions\n",
        "\n",
        "\t\tc_2 is the constant $c_2$ from the Strong Wolfe conditions\n",
        "\n",
        "\tReturns:\n",
        "\t\tA suitable value for $\\alpha$ that meets the Strong Wolfe\n",
        "\t\t\tconditions, a floating point scalar.\n",
        "\n",
        "\tSide Effects:\n",
        "\t\tNone\n",
        "\t\"\"\"\n",
        "\n",
        "\titeration = 1\n",
        "\tmax_iterations = 100\n",
        "\twhile (iteration <= max_iterations):\n",
        "\t\titeration += 1\n",
        "\n",
        "\t\t# interpolate using bisection to find a trial step\n",
        "\t\t# length $\\alpha_j$ between $\\alpha_{lo}$ and $\\alpha_{hi}$\n",
        "\t\talpha_j = (alpha_hi + alpha_lo) / 2.0\n",
        "\n",
        "\t\t# on the rare occasion that $\\alpha_{hi} = \\alpha_{lo}$ we actually go outside that range to find a usable value.\n",
        "\t\tif alpha_hi == alpha_lo:\n",
        "\t\t\talpha_j = alpha_hi / 2.0\n",
        "\n",
        "\t\t# Evaluate $\\phi(\\alpha_j)$\n",
        "\t\tphi_alpha_j = fobj.f(x_k + alpha_j*p_k)\n",
        "\n",
        "\t\t# if $\\phi(\\alpha_j) > \\phi(0) + c_1\\alpha_j\\phi'(0)$ or $\\phi(\\alpha_j) \\geq \\phi(\\alpha_{lo})$\n",
        "\t\tif phi_alpha_j > phi_0 + c_1*alpha_j*phi_prime_0 or phi_alpha_j >= fobj.f(x_k + alpha_lo*p_k):\n",
        "\t\t\t# $\\alpha_{hi} \\leftarrow \\alpha_j$\n",
        "\t\t\talpha_hi = alpha_j\n",
        "\t\telse:\n",
        "\t\t\t# Evaluate $\\phi'(\\alpha_j)$\n",
        "\t\t\tphi_prime_alpha_j = dot(fobj.grad_f(x_k + alpha_j*p_k), p_k)\n",
        "\n",
        "\t\t\t# if $|\\phi'(\\alpha_j)| \\leq -c_2 \\phi'(0)$\n",
        "\t\t\tif abs(phi_prime_alpha_j) <= -c_2*phi_prime_0:\n",
        "\t\t\t\t# set $\\alpha_\\ast \\leftarrow \\alpha_j$ and stop\n",
        "\t\t\t\treturn alpha_j\n",
        "\n",
        "\t\t\t# if $\\phi'(\\alpha_j)(\\alpha_{hi} - \\alpha_{lo}) \\geq 0$\n",
        "\t\t\tif phi_prime_alpha_j*(alpha_hi - alpha_lo) >= 0:\n",
        "\t\t\t\t# $\\alpha_{hi} \\leftarrow \\alpha_{lo}$\n",
        "\t\t\t\talpha_hi = alpha_lo\n",
        "\n",
        "\t\t\t# $\\alpha_{lo} \\leftarrow \\alpha_j$\n",
        "\t\t\talpha_lo = alpha_j\n",
        "\n",
        "\t\tprint(\"zoom: max iterations exited!\")\n",
        "\t\tquit()\n",
        "\n",
        "\n",
        "\n",
        "def StrongWolfe(fobj, x_k, p_k, alpha_max):\n",
        "\t\"\"\"\n",
        "\tThis is an implementation of Algorithm 3.2, page 59 of Nocedal and Wright,\n",
        "\ta line search algorithm that meets the Strong Wolfe conditions.\n",
        "\n",
        "\tInputs:\n",
        "\t\tfobj is an objective function object\n",
        "\n",
        "\t\tx_k is the $x_k$ value, the staring point to be used for the\n",
        "\t\t\tsearch for $\\alpha$, and is a Numeric array\n",
        "\n",
        "\t\tp_k is the $p_k$ value, the search direction to be used for the\n",
        "\t\t\tsearch for $\\alpha$, and is a Numeric array\n",
        "\n",
        "\t\talpha_max is some maximum value that the algorithm will not exceed.\n",
        "\n",
        "\tReturns:\n",
        "\t\tA suitable value for $\\alpha$, a floating point scalar, that\n",
        "\t\t\tsatisfiies the Strong Wolfe Conditions\n",
        "\n",
        "\tSide Effects:\n",
        "\t\tNone\n",
        "\t\"\"\"\n",
        "\t# set $\\alpha_0 \\leftarrow 0$, choose $\\alpha_1 > 0$ and $\\alpha_{max}$\n",
        "\talpha_i_minus_1 = 0\n",
        "\talpha_i = alpha_max\n",
        "\n",
        "\t# $i \\leftarrow 1$\n",
        "\ti = 1\n",
        "\n",
        "\t# pre-eval $\\phi(0)$\n",
        "\tphi_0 = fobj.f(x_k)\n",
        "\n",
        "\t# from (A.16) we have $\\phi'(\\alpha) = \\nabla f(x_k + \\alpha p_k)^T p_k$\n",
        "\t# so $\\phi'(0) = \\nabla f_k^Tp_k$\n",
        "\tphi_prime_0 = dot(fobj.grad_f(x_k), p_k)\n",
        "\n",
        "\tphi_alpha_i_minus_1 = phi_0\n",
        "\n",
        "\t# intelligent values for these found on pg 37-38\n",
        "\tc_1 = 10e-4\n",
        "\tc_2 = 0.99\n",
        "\n",
        "\twhile 1:\n",
        "\t\t# Evaluate $\\phi(\\alpha_i)$\n",
        "\t\tphi_alpha_i = fobj.f(x_k + alpha_i * p_k)\n",
        "\n",
        "\t\t# if $\\phi(\\alpha_i) > \\phi(0) + c_1\\alpha_i\\phi'(0)$ or $[\\phi(\\alpha_i) \\geq \\phi(\\alpha_{i-1})$ and $i > 1]$\n",
        "\t\tif phi_alpha_i > phi_0 + c_1*alpha_i*phi_prime_0 or (phi_alpha_i >= phi_alpha_i_minus_1 and i > 1):\n",
        "\t\t\t# $\\alpha_\\ast \\leftarrow$ zoom$(\\alpha_{i-1}, \\alpha_i)$ and stop;\n",
        "\t\t\treturn zoom(fobj, x_k, p_k, alpha_i_minus_1, alpha_i, phi_0, phi_prime_0, c_1, c_2)\n",
        "\n",
        "\t\t# Evaluate $\\phi'(\\alpha_i) = \\nabla f(x_k + \\alpha_i p_k)^Tp_k$\n",
        "\t\tphi_prime_alpha_i = dot(fobj.grad_f(x_k + alpha_i*p_k), p_k)\n",
        "\n",
        "\t\t# if $|\\phi'(a_i)| \\leq -c_2\\phi'(0)$\n",
        "\t\tif abs(phi_prime_alpha_i) <= -c_2*phi_prime_0:\n",
        "\t\t\t# set $\\alpha_\\ast \\leftarrow \\alpha_i$ and stop;\n",
        "\t\t\treturn alpha_i\n",
        "\n",
        "\t\t# if $\\phi'(\\alpha_i) \\geq 0$\n",
        "\t\t\t# set $\\alpha_\\ast \\leftarrow$ zoom$(\\alpha_i, \\alpha_{i-1})$ and stop\n",
        "\t\t\treturn zoom(fobj, x_k, p_k, alpha_i, alpha_i_minus_1, phi_0, phi_prime_0, c_1, c_2)\n",
        "\n",
        "\t\t# advance our variables\n",
        "\t\talpha_i_minus_1 = alpha_i\n",
        "\t\tphi_alpha_i_minus_1 = phi_alpha_i\n",
        "\n",
        "\t\t# Choose $\\alpha_{i+1} \\in (\\alpha_i, \\alpha_{max})$\n",
        "\t\talpha_i = min(alpha_max, 1.1*alpha_i)\n",
        "\n",
        "\t\t# $i \\leftarrow i + 1$\n",
        "\t\ti += 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9IqPwg-2Mog",
        "outputId": "e1486a87-9a11-4f6a-c4c4-fd71727ce489"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:18: SyntaxWarning: invalid escape sequence '\\p'\n",
            "<>:18: SyntaxWarning: invalid escape sequence '\\p'\n",
            "/tmp/ipython-input-203587291.py:18: SyntaxWarning: invalid escape sequence '\\p'\n",
            "  phi_0 is $\\phi(0)$, a floating point scalar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BFGS Line Search"
      ],
      "metadata": {
        "id": "W_1-q_yg2nxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def BFGSLineSearch(x_0, alpha_0, f_alpha_k, fobj, epsilon, H_0):\n",
        "\t\"\"\"\n",
        "\tAn implementation of the modified BFGS algorithm\n",
        "\tfrom Nocedal and Wright page 198.\n",
        "\n",
        "\tInputs:\n",
        "\t\tx_0 is a Numeric array giving the initial value $x_0$\n",
        "\n",
        "\t\talpha_0 is the $\\alpha_0$, the inital value for $\\alpha$.\n",
        "\n",
        "\t\tf_alpha_k is the line search function, it takes a fobj,\n",
        "\t\t\tx_k, p_k, and alpha_max and returns an alpha value\n",
        "\t\t\tfor the current iteration.  Recommend StrongWolfe above.\n",
        "\n",
        "\t\tfobj is the objective function object\n",
        "\n",
        "\t\tepsilon is the tolerance value of $\\epsilon$, the function\n",
        "\t\t\treturns when $\\|\\nabla f_k\\| > \\epsilon$\n",
        "\n",
        "\t\tH_0 is the inital inverse Hessian estimate $H_0$\n",
        "\n",
        "\tOutputs:\n",
        "\t\tReturns the computed value of the stationary point of $f$, $x^\\ast$,\n",
        "\t\t\twithin tolerance $\\epsilon$\n",
        "\n",
        "\tSide Effects:\n",
        "\t\tReports to stdout at each iteration the value of $\\alpha_k$,\n",
        "\t\t\t$x_k$, the number of function evaluations, and\n",
        "\t\t\tif the product $y_k^Ts_k$ is positive.\n",
        "\t\"\"\"\n",
        "\t# $k \\leftarrow 0$\n",
        "\tk = 0\n",
        "\n",
        "\tx_k = x_0\n",
        "\tH_k = H_0\n",
        "\n",
        "\t# print out our table header\n",
        "\tprint ('\\n\\nBFGS Line Search on %s' \\\n",
        "\t\t% fobj.descriptive_name())\n",
        "\tprint ('alpha_0 = %f, x_0 = %s' % (alpha_0, x_0))\n",
        "\tprint ('iteration\\talpha_k\\t\\tx_k\\t\\t\\t\\tf_evals\\tyk*sk positive?')\n",
        "\n",
        "\t# while $\\|\\nabla f_k\\|$\n",
        "\twhile sqrt(dot(fobj.grad_f(x_k), fobj.grad_f(x_k))) > epsilon:\n",
        "\t\t# $p_k = -H_k \\nabla f_k$\n",
        "\t\tp_k = -1.0 * matrixmultiply(H_k, fobj.grad_f(x_k))\n",
        "\n",
        "\t\t# apply the line search technique supplied\n",
        "\t\talpha_k = f_alpha_k(fobj, x_k, p_k, alpha_0)\n",
        "\n",
        "\t\t# $x_{k+1} = x_k + \\alpha p_k$\n",
        "\t\tx_k_plus_1 = x_k + (alpha_k * p_k)\n",
        "\n",
        "\t\t# $s_k = x_{k+1} - x_k$\n",
        "\t\ts_k = x_k_plus_1 - x_k\n",
        "\n",
        "\t\t# $y_k = \\nabla f_{k+1} - \\nabla f_k$\n",
        "\t\ty_k = fobj.grad_f(x_k_plus_1) - fobj.grad_f(x_k)\n",
        "\n",
        "\t\t# $\\rho_k = (y_k^T s_k)^{-1}$\n",
        "\t\trho_k = 1.0 / dot(y_k, s_k)\n",
        "\n",
        "\t\t# $H_{k+1} = (I - \\rho_k s_k y_k^T) H_k (I - \\rho_k y_k s_k^T) + p_k s_k s_k^T$\n",
        "\t\tH_k_plus_1 = matrixmultiply(\\\n",
        "\t\t\tmatrixmultiply(identity(x_0.shape[0]) - \\\n",
        "\t\t\t\trho_k*matrixmultiply(\\\n",
        "\t\t\t\ts_k, transpose(y_k)), H_k),\\\n",
        "\t\t\tidentity(x_0.shape[0]) \\\n",
        "\t\t\t - rho_k*matrixmultiply(y_k, transpose(s_k))) \\\n",
        "\t\t\t+ matrixmultiply(matrixmultiply(p_k, s_k), \\\n",
        "\t\t\t\t\ttranspose(s_k))\n",
        "\n",
        "\t\t# $k \\leftarrow k+1$\n",
        "\t\tk = k + 1\n",
        "\n",
        "\t\t# advance the values\n",
        "\t\tx_k = x_k_plus_1\n",
        "\t\tH_k = H_k_plus_1\n",
        "\n",
        "\t\t# check and make sure that $y_k^Ts_k$ is positive\n",
        "\t\tif dot(y_k, s_k) > 0:\n",
        "\t\t\ty_k_s_k_positive = 'yes'\n",
        "\t\telse:\n",
        "\t\t\ty_k_s_k_positive = 'no'\n",
        "\n",
        "\t\t# print out a line of the table\n",
        "\t\tprint('%d\\t\\t%f\\t%s\\t%d\\t%s' \\\n",
        "\t\t\t% (k, alpha_k, x_k, fobj.eval_count(), y_k_s_k_positive))\n",
        "\n",
        "\t\tsys.stdout.flush()\n",
        "\n",
        "\treturn x_k\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haM6jo362sc-",
        "outputId": "5aee8efd-843a-43f3-dafe-de5718598900"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:17: SyntaxWarning: invalid escape sequence '\\e'\n",
            "<>:17: SyntaxWarning: invalid escape sequence '\\e'\n",
            "/tmp/ipython-input-1516872251.py:17: SyntaxWarning: invalid escape sequence '\\e'\n",
            "  epsilon is the tolerance value of $\\epsilon$, the function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit tests for Line Search"
      ],
      "metadata": {
        "id": "j33IG8Oi4wJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#####################\n",
        "# unit testing code #\n",
        "#####################\n",
        "\n",
        "class BFGSTestCase(unittest.TestCase):\n",
        "\tdef testDescentFunction(self):\n",
        "\t\t# Generate a random number from a uniform distribution between -100 and +100\n",
        "\t\t# loc=-100, scale= +100 - (-100) = +200\n",
        "\t\ta = uniform.rvs(loc=-100.0, scale=200.0, size=1)[0]\n",
        "\t\tb = uniform.rvs(loc=-100.0, scale=200.0, size=1)[0]\n",
        "\t\tx_0 = array((a, b))\n",
        "\t\talpha_0 = 1.0\n",
        "\n",
        "\t\tf1 = testObjFunc()\n",
        "\n",
        "\t\tx_star = BFGSLineSearch(x_0, alpha_0, \\\n",
        "\t\t\t\t\tStrongWolfe,\n",
        "\t\t\t\t\tf1, \\\n",
        "\t\t\t\t\t9.9e-13, \\\n",
        "\t\t\t\t\tlinalg.inv(f1.hessian_f(x_0)))\n",
        "\t\tassert np.allclose(x_star, array((0.0, 0.0)))\n",
        "\n",
        "\t\t# Generate a random number from a uniform distribution between -100 and +100\n",
        "\t\t# loc=-100, scale= +100 - (-100) = +200\n",
        "\t\ta = uniform.rvs(loc=-100.0, scale=200.0, size=1)[0]\n",
        "\t\tb = uniform.rvs(loc=-100.0, scale=200.0, size=1)[0]\n",
        "\t\tx_0 = array((a, b))\n",
        "\t\talpha_0 = 1.0\n",
        "\n",
        "\t\tf2 = testSecondFunc()\n",
        "\n",
        "\t\tx_star = BFGSLineSearch(x_0, alpha_0, \\\n",
        "\t\t\t\t\tStrongWolfe, \\\n",
        "\t\t\t\t\tf2, \\\n",
        "\t\t\t\t\t9.9e-13, \\\n",
        "\t\t\t\t\tlinalg.inv(f2.hessian_f(x_0)))\n",
        "\t\tassert np.allclose(x_star, array((1.0, 1.0)))\n",
        "\n",
        "class SteepestDescentTestCase(unittest.TestCase):\n",
        "\tdef testDescentFunction(self):\n",
        "\t\t# Generate a random number from a uniform distribution between -100 and +100\n",
        "\t\t# loc=-100, scale= +100 - (-100) = +200\n",
        "\t\ta = uniform.rvs(loc=-100.0, scale=200.0, size=1)[0]\n",
        "\t\tb = uniform.rvs(loc=-100.0, scale=200.0, size=1)[0]\n",
        "\t\tx_0 = array((a, b))\n",
        "\t\talpha_0 = 1.0\n",
        "\n",
        "\t\tx_star = SteepestDescentLineSearch(x_0, alpha_0, \\\n",
        "\t\t\t\t  BacktrackingAlpha, \\\n",
        "\t\t\t\t  testObjFunc(), \\\n",
        "\t\t\t\t  9.9e-13)\n",
        "\t\tassert np.allclose(x_star, array((0.0, 0.0)))\n",
        "\n",
        "class NewtonsMethodTestCase(unittest.TestCase):\n",
        "\tdef testDescentFunction(self):\n",
        "\t\t# Generate a random number from a uniform distribution between -100 and +100\n",
        "\t\t# loc=-100, scale= +100 - (-100) = +200\n",
        "\t\ta = uniform.rvs(loc=-100.0, scale=200.0, size=1)[0]\n",
        "\t\tb = uniform.rvs(loc=-100.0, scale=200.0, size=1)[0]\n",
        "\t\tx_0 = array((a, b))\n",
        "\t\talpha_0 = 1.0\n",
        "\n",
        "\t\tx_star = NewtonsMethodLineSearch(x_0, alpha_0, \\\n",
        "\t\t\t\tBacktrackingAlpha, \\\n",
        "\t\t\t\ttestObjFunc(), \\\n",
        "\t\t\t\t9.9e-13)\n",
        "\t\tassert np.allclose(x_star, array((0.0, 0.0)))\n",
        "\n",
        "\t\t# Generate a random number from a uniform distribution between -100 and +100\n",
        "\t\t# loc=-100, scale= +100 - (-100) = +200\n",
        "\t\ta = uniform.rvs(loc=-100.0, scale=200.0, size=1)[0]\n",
        "\t\tb = uniform.rvs(loc=-100.0, scale=200.0, size=1)[0]\n",
        "\t\tx_0 = array((a, b))\n",
        "\t\talpha_0 = 1.0\n",
        "\n",
        "\t\tx_star = NewtonsMethodLineSearch(x_0, alpha_0, \\\n",
        "\t\t\t\t BacktrackingAlpha, \\\n",
        "\t\t\t\t testSecondFunc(), \\\n",
        "\t\t\t\t 9.9e-13)\n",
        "\t\tassert np.allclose(x_star, array((1.0, 1.0)))"
      ],
      "metadata": {
        "id": "2S2WWijP4t7K"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJGhUHxW5HZQ",
        "outputId": "b996d041-a780-4b51-8e51-82f5725e80b1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "BFGS Line Search on test function\n",
            "alpha_0 = 1.000000, x_0 = [-48.1644201  -78.60728299]\n",
            "iteration\talpha_k\t\tx_k\t\t\t\tf_evals\tyk*sk positive?\n",
            "1\t\t1.000000\t[0. 0.]\t5\tyes\n",
            "\n",
            "\n",
            "BFGS Line Search on a second test function\n",
            "alpha_0 = 1.000000, x_0 = [-10.24761549  19.55442117]\n",
            "iteration\talpha_k\t\tx_k\t\t\t\tf_evals\tyk*sk positive?\n",
            "1\t\t1.000000\t[1. 1.]\t5\tyes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "."
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Newton's Method Line Search on test function\n",
            "alpha_0 = 1.000000, x_0 = [ 69.37714911 -30.60070131]\n",
            "iteration alpha_k   x_k       f evals   \n",
            "         1    1.0000[0. 0.]         4\n",
            "\n",
            "\n",
            "Newton's Method Line Search on a second test function\n",
            "alpha_0 = 1.000000, x_0 = [-51.57997261  47.00097139]\n",
            "iteration alpha_k   x_k       f evals   \n",
            "         1    1.0000[1. 1.]         4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "..........."
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "\n",
            "--- Debugging testProblem4_1Hessian ---\n",
            "Input x: [0. 0.]\n",
            "Calculated Hessian: [[42. -0.]\n",
            " [-0. 20.]]\n",
            "Expected Hessian: [[42  0]\n",
            " [ 0 20]]\n",
            "Input x: [1. 0.]\n",
            "Calculated Hessian: [[ 42. -40.]\n",
            " [-40.  20.]]\n",
            "Expected Hessian: [[ 42 -40]\n",
            " [-40  20]]\n",
            "Input x: [0. 1.]\n",
            "Calculated Hessian: [[ 2. -0.]\n",
            " [-0. 20.]]\n",
            "Expected Hessian: [[ 2  0]\n",
            " [ 0 20]]\n",
            "Input x: [1. 1.]\n",
            "Calculated Hessian: [[  2. -40.]\n",
            " [-40.  20.]]\n",
            "Expected Hessian: [[  2 -40]\n",
            " [-40  20]]\n",
            "\n",
            "\n",
            "Steepest Descent Line Search on test function\n",
            "alpha_0 = 1.000000, x_0 = [ 36.35211959 -79.1019902 ]\n",
            "iteration\talpha_k\t\tx_k\t\t\t\tf evals\n",
            "1\t\t0.500000\t[0. 0.]\t4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 13 tests in 0.040s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7f36b4e1a240>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trust Region"
      ],
      "metadata": {
        "id": "JWWc2aCMBLV1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## bisection(.): Root finding within a and b."
      ],
      "metadata": {
        "id": "x5823zapBUA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bisection(fobj, a, b, xtol=9.9e-13, maxiter=1000):\n",
        "\t\"\"\"\n",
        "\tFind the root using the bisection method.  I copied part of the\n",
        "\tfunction signature from scipy.optimize.bisect(), which is\n",
        "\tscipy's version of this function\n",
        "\n",
        "\tInputs:\n",
        "\t\tfobj:  object with member f, a one dimensional function taking a number\n",
        "\n",
        "\t\ta:  Number, one end of the bracketing interval\n",
        "\n",
        "\t\tb:  Number, other end of the bracketing interval\n",
        "\n",
        "\t\txtol: Number, allowable tolerance on f for solution\n",
        "\t\t\tshould be $\\geq$ 0\n",
        "\n",
        "\t\tmaxiter: Number, maximum of iterations allowable\n",
        "\n",
        "\tReturns:\n",
        "\t\tThe found solution, or NaN if no solution was found\n",
        "\t\"\"\"\n",
        "\titer_count = 0\n",
        "\twhile True:\n",
        "\t\tx_mid = (b + a) / 2.0\n",
        "\t\tf_x_mid = fobj.f(x_mid)\n",
        "\t\titer_count += 1\n",
        "\t\tif sign(f_x_mid) != sign(fobj.f(a)):\n",
        "\t\t\tb = x_mid\n",
        "\t\telif sign(f_x_mid) != sign(fobj.f(b)):\n",
        "\t\t\ta = x_mid\n",
        "\t\telse:\n",
        "\t\t\treturn NaN\n",
        "\t\tmaxiter -= 1\n",
        "\t\tif math.fabs(f_x_mid) < xtol or maxiter == 0:\n",
        "\t\t\tbreak\n",
        "\n",
        "\tif maxiter == 0:\n",
        "\t\tx_mid = NaN\n",
        "\n",
        "\treturn x_mid"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hb-nO_4oBXWz",
        "outputId": "cbb8cdce-c9c7-4f35-95e7-09991b967390"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:15: SyntaxWarning: invalid escape sequence '\\g'\n",
            "<>:15: SyntaxWarning: invalid escape sequence '\\g'\n",
            "/tmp/ipython-input-1902976910.py:15: SyntaxWarning: invalid escape sequence '\\g'\n",
            "  should be $\\geq$ 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tau Minimizer"
      ],
      "metadata": {
        "id": "E2bm_4B1Bqgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TauMinimizer:\n",
        "\t\"\"\"\n",
        "\tA small class that defines a function $f(\\tau)$ with\n",
        "\n",
        "\t$f(\\tau) = (p_j + \\tau d_j)^T(p_j + \\tau d_j) - \\Delta^2$\n",
        "\t\"\"\"\n",
        "\tdef __init__(self, p_j, d_j, Delta):\n",
        "\t\t\"\"\"\n",
        "\t\tHere in the constructor we define the values for\n",
        "\t\t$p_j$, $d_j$, and $\\Delta$ for use in our function.\n",
        "\t\t\"\"\"\n",
        "\t\tself.p_j = p_j\n",
        "\t\tself.d_j = d_j\n",
        "\t\tself.Delta = Delta\n",
        "\n",
        "\tdef f(self, tau):\n",
        "\t\t\"\"\"\n",
        "\t\tReturn the function with the parameters given in the ctor\n",
        "\t\tevaluated at tau = $\\tau$\n",
        "\t\t\"\"\"\n",
        "\t\treturn dot(self.p_j + tau*self.d_j, self.p_j + tau*self.d_j) - self.Delta**2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-hRFGacBvBH",
        "outputId": "70db45a3-c852-4c72-d605-c60e8cf61811"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:5: SyntaxWarning: invalid escape sequence '\\D'\n",
            "<>:10: SyntaxWarning: invalid escape sequence '\\D'\n",
            "<>:5: SyntaxWarning: invalid escape sequence '\\D'\n",
            "<>:10: SyntaxWarning: invalid escape sequence '\\D'\n",
            "/tmp/ipython-input-4140902362.py:5: SyntaxWarning: invalid escape sequence '\\D'\n",
            "  $f(\\tau) = (p_j + \\tau d_j)^T(p_j + \\tau d_j) - \\Delta^2$\n",
            "/tmp/ipython-input-4140902362.py:10: SyntaxWarning: invalid escape sequence '\\D'\n",
            "  $p_j$, $d_j$, and $\\Delta$ for use in our function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CG Steihaug"
      ],
      "metadata": {
        "id": "hxPVtUK6B1pq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def CG_Steihaug(fobj, epsilon, Delta, B_k, g):\n",
        "\t\"\"\"\n",
        "\tNote that fobj.n should be set to something useful\n",
        "\tbefore function entry.  Calculates the $p_k$ for the\n",
        "\tnext iteration.  Based on algorithm 4.3 on page 75\n",
        "\tof Nocedal and Wright.\n",
        "\n",
        "\tInputs:\n",
        "\t\tfobj is the function object, used mostly for n\n",
        "\n",
        "\t\tepsilon is the tolerance\n",
        "\n",
        "\t\tDelta is the trust region size\n",
        "\n",
        "\t\tB_k is the $B_k$, usually the exact hessian at $x_k$\n",
        "\n",
        "\t\tg is the gradient of f evaluated at $x_k$\n",
        "\n",
        "\tOutputs:\n",
        "\t\treturns a tuple with the new $p_k$ and 4 integer values,\n",
        "\t\t3 of which will be zero and one of which will be one.  They\n",
        "\t\tare in order:\n",
        "\t\ttolerance - meaning that the p_j matches the tolernace and is good\n",
        "\t\tboundary - meaning that p_j was outside our trust region\n",
        "\t\tnegative - meaning that the algorithm encountered negative curvature\n",
        "\t\tmaxiter - meaning that the looped maxed out at n\n",
        "\n",
        "\tSide Effects:\n",
        "\t\tNone\n",
        "\t\"\"\"\n",
        "\t# make a $p_j$ with dimension of the fobj\n",
        "\t# set $p_0 = p_j = p_0$\n",
        "\tp_j = zeros((fobj.n))\n",
        "\n",
        "\t# set $r_0 = r_j = g$\n",
        "\tr_j = g\n",
        "\n",
        "\t# set $d_0 = d_j = -r_0$\n",
        "\td_j = -r_j\n",
        "\n",
        "\t# if $\\|r_0\\| < \\epsilon$\n",
        "\tmag_r_0 = math.sqrt(dot(r_j, r_j))\n",
        "\tif mag_r_0 < epsilon:\n",
        "\t\t# lucked out on choice of p_0\n",
        "\t\treturn (p_j, 1, 0, 0, 0)\n",
        "\n",
        "\t# for $j = 0, 1, 2, \\ldots$ dimension of $B$\n",
        "\tfor j in range(0, fobj.n):\n",
        "\t\t# if $d_j^TB_kd_j \\leq 0$\n",
        "\t\tif MatQuad(d_j, B_k, d_j) <= 0:\n",
        "\t\t\t# encountered negative curvature\n",
        "\t\t\t# Find $\\tau$ such that $p = p_j + \\tau d_j$ minimizes\n",
        "\t\t\t# $m(p)$ in (4.9) and satisfies $\\|p\\| = \\Delta$\n",
        "\t\t\ttm = TauMinimizer(p_j, d_j, Delta)\n",
        "\t\t\ttau_1 = bisection(tm, 0.0, 200.0*Delta)\n",
        "\t\t\ttau_2 = bisection(tm, 0.0, -200.0*Delta)\n",
        "\t\t\tp_1 = p_j + tau_1 * d_j\n",
        "\t\t\tp_2 = p_j + tau_2 * d_j\n",
        "\t\t\tif (p_1 < p_2):\n",
        "\t\t\t\treturn (p_1, 0, 0, 1, 0)\n",
        "\t\t\telse:\n",
        "\t\t\t\treturn (p_2, 0, 0, 1, 0)\n",
        "\n",
        "\t\t# $\\alpha_j = r_j^Tr_j / d_j^TB_kd_j$\n",
        "\t\talpha_j = dot(r_j, r_j) / MatQuad(d_j, B_k, d_j)\n",
        "\n",
        "\t\t# $p_{j+1} = p_j + \\alpha_jd_j$\n",
        "\t\tp_j_plus_1 = p_j + alpha_j * d_j\n",
        "\n",
        "\t\t# if $\\|p_{j+1}\\| \\geq \\Delta$\n",
        "\t\tif math.sqrt(dot(p_j_plus_1, p_j_plus_1)) >= Delta:\n",
        "\t\t\t# reached trust region boundary\n",
        "\t\t\ttm = TauMinimizer(p_j, d_j, Delta)\n",
        "\t\t\ttau_1 = bisection(tm, 0.0, 200.0*Delta)\n",
        "\t\t\ttau_2 = bisection(tm, 0.0, -200.0*Delta)\n",
        "\t\t\tif tau_1 > 0:\n",
        "\t\t\t\treturn (p_j + tau_1*d_j, 0, 1, 0, 0)\n",
        "\t\t\telse:\n",
        "\t\t\t\treturn (p_j + tau_2*d_j, 0, 1, 0, 0)\n",
        "\n",
        "\t\t# $r_{j+1} = r_j + \\alpha_jB_kd_j$\n",
        "\t\tr_j_plus_1 = r_j + alpha_j*matrixmultiply(B_k, d_j)\n",
        "\n",
        "\t\t# if $\\|r_{j+1}\\| < \\epsilon\\|r_0\\|$\n",
        "\t\tif math.sqrt(dot(r_j_plus_1, r_j_plus_1)) < epsilon*mag_r_0:\n",
        "\t\t\t#print 'met stopping test'\n",
        "\t\t\treturn (p_j_plus_1, 1, 0, 0, 0)\n",
        "\n",
        "\t\t# $\\beta_{j+1} = r_{j+1}^Tr_{j+1}/r_j^Tr_j$\n",
        "\t\tbeta_j_plus_1 = dot(r_j_plus_1, r_j_plus_1) / dot(r_j, r_j)\n",
        "\n",
        "\t\t# $j_{j+1} = r_{j+1} + \\beta_{j+1}d_j$\n",
        "\t\td_j_plus_1 = -r_j_plus_1 + beta_j_plus_1*d_j\n",
        "\n",
        "\t\t# advance all the plus ones\n",
        "\t\tp_j = p_j_plus_1\n",
        "\t\tr_j = r_j_plus_1\n",
        "\t\td_j = d_j_plus_1\n",
        "\n",
        "\t# if we get here we maxed out of the loop\n",
        "\t# maxed out on loop\n",
        "\treturn (p_j, 0, 0, 0, 1)"
      ],
      "metadata": {
        "id": "RCikM-rdB6qb"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Local quadratic model"
      ],
      "metadata": {
        "id": "3-WGESUXCEYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def localmodel(fobj, x_k, p):\n",
        "\t\"\"\"\n",
        "\tReturns\n",
        "\n",
        "\n",
        "\t\t$f(x_k) + \\nabla f(x_k)^T p + \\frac{1}{2}p^T\\nabla^2f(x_k) p$\n",
        "\n",
        "\t\twith $B$ the exact hessian\n",
        "\t\"\"\"\n",
        "\treturn fobj.f(x_k) + dot(fobj.grad_f(x_k), p) \\\n",
        "\t\t+ 0.5*MatQuad(p, fobj.hessian_f(x_k), p)\n"
      ],
      "metadata": {
        "id": "ZFejYfaWCMWa"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trust Region algorithm\n"
      ],
      "metadata": {
        "id": "cosAy1ZcCaio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def TrustRegion(x_0, Delta_Bar, Delta_0, eta, fobj, maxiter):\n",
        "\t\"\"\"\n",
        "\tThis is an implementation of algorithm 4.1 in Nocedal\n",
        "\tand Wright, page 68.\n",
        "\n",
        "\tInputs:\n",
        "\t\tx_0 is the $x_0$ value, an initial value for $x$\n",
        "\n",
        "\t\tDelta_Bar is the $\\overline{\\Delta}$ value,\n",
        "\t\t\tthe max value for Delta\n",
        "\n",
        "\t\tDelta_0 is the $\\Delta_0$ value, the inital radius\n",
        "\t\t\tof the trust region\n",
        "\n",
        "\t\teta is the $\\eta$ value, the minimum acceptable\n",
        "\t\t\treduction of $f_k$\n",
        "\n",
        "\t\tfobj is the function object\n",
        "\n",
        "\t\tmaxiter is the maximum number of iterations accepted.\n",
        "\n",
        "\tOutputs:\n",
        "\t\tReturns the stationary point of $f$, $x^\\ast$.\n",
        "\n",
        "\tSide Effects:\n",
        "\t\tPrints the counts of the different return states of\n",
        "\t\tCG_Steihaug at the end of the algorithm.\n",
        "\t\"\"\"\n",
        "\tx_k = x_0\n",
        "\tDelta_k = Delta_0\n",
        "\n",
        "\t# we keep counters of the exit conditions of CG_Steihaug\n",
        "\ttol = 0\n",
        "\tbound = 0\n",
        "\tneg = 0\n",
        "\tmaxed = 0\n",
        "\n",
        "\tfor k in range(0, maxiter):\n",
        "\t\tp_k, t, b, n, m = CG_Steihaug(fobj, 9.9e-13, Delta_k, fobj.hessian_f(x_k), fobj.grad_f(x_k))\n",
        "\n",
        "\t\t# update the exit condition counters\n",
        "\t\ttol += t\n",
        "\t\tbound += b\n",
        "\t\tneg += n\n",
        "\t\tmaxed += m\n",
        "\n",
        "\t\t# $\\rho_k = \\frac{f(x_k) - f(x_k + p_k)}{m_k(0) - m_k(p_k)}$\n",
        "\t\trho_k = (fobj.f(x_k) - fobj.f(x_k + p_k)) \\\n",
        "\t\t\t / (localmodel(fobj, x_k, zeros((fobj.n))) \\\n",
        "\t\t\t    - localmodel(fobj, x_k, p_k))\n",
        "\n",
        "\t\t# precaculate $\\|p_k\\|$\n",
        "\t\tmag_p_k = math.sqrt(dot(p_k, p_k))\n",
        "\n",
        "\t\t# if $\\rho_k < \\frac{1}{4}$\n",
        "\t\tif rho_k < 0.25:\n",
        "\t\t\t# $\\Delta_{k+1} = \\frac{1}{4}\\|p_k\\|$\n",
        "\t\t\tDelta_k = 0.25*mag_p_k\n",
        "\t\telse:\n",
        "\t\t\t# if $\\rho_k > \\frac{3}{4}$ and $\\|p_k\\| = \\Delta_k$\n",
        "\t\t\tif rho_k > 0.75 and mag_p_k - Delta_k < 9.9e-13:\n",
        "\t\t\t\tDelta_k = 2*Delta_k\n",
        "\t\t\t\tif Delta_k > Delta_Bar:\n",
        "\t\t\t\t\tDelta_k = Delta_Bar\n",
        "\t\t# if $\\rho_k > \\eta$\n",
        "\t\tif rho_k > eta:\n",
        "\t\t\t# $x_{k+1} = x_k + p_k$\n",
        "\t\t\tx_k = x_k + p_k\n",
        "\n",
        "\t\t# break out if we've found a stationary point within tolerance\n",
        "\t\tif math.sqrt(dot(fobj.grad_f(x_k), fobj.grad_f(x_k))) < 9.9e-13:\n",
        "\t\t\tbreak\n",
        "\n",
        "\tprint ('tolerance met: %d, trust region boundary: %d, negative curvature: %d, maximum iterations: %d' % \\\n",
        "\t    (tol, bound, neg, maxed))\n",
        "\n",
        "\tprint ('Number of function evaluations: %d' % fobj.eval_count())\n",
        "\n",
        "\treturn x_k"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nH7RhzaWCzpE",
        "outputId": "0cf62137-d2f7-4b40-9088-4f21af1afe2d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:9: SyntaxWarning: invalid escape sequence '\\o'\n",
            "<>:9: SyntaxWarning: invalid escape sequence '\\o'\n",
            "/tmp/ipython-input-2800652159.py:9: SyntaxWarning: invalid escape sequence '\\o'\n",
            "  Delta_Bar is the $\\overline{\\Delta}$ value,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit testing code for trust region"
      ],
      "metadata": {
        "id": "g4g2uFP4C_lK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TrustRegionTestCase(unittest.TestCase):\n",
        "\tdef testTrustRegion(self):\n",
        "    # Generate a random number from a uniform distribution between -100 and +100\n",
        "\t\t# loc=-100, scale= +100 - (-100) = +200\n",
        "\t\ta = uniform.rvs(loc=-100.0, scale=200.0, size=1)[0]\n",
        "\t\tb = uniform.rvs(loc=-100.0, scale=200.0, size=1)[0]\n",
        "\t\tx_0 = array((a, b))\n",
        "\t\talpha_0 = 1.0\n",
        "\t\tf1 = testObjFunc()\n",
        "\n",
        "\t\tx_star = TrustRegion(x_0, 10.0, 1.0, 0.9, f1, 1000)\n",
        "\t\tassert np.allclose(x_star, array((0.0, 0.0)))\n",
        "\n",
        "    # Generate a random number from a uniform distribution between -100 and +100\n",
        "\t\t# loc=-100, scale= +100 - (-100) = +200\n",
        "\t\ta = uniform.rvs(loc=-100.0, scale=200.0, size=1)[0]\n",
        "\t\tb = uniform.rvs(loc=-100.0, scale=200.0, size=1)[0]\n",
        "\t\tx_0 = array((a, b))\n",
        "\t\talpha_0 = 1.0\n",
        "\t\tf2 = testSecondFunc()\n",
        "\n",
        "\t\tx_star = TrustRegion(x_0, 10.0, 1.0, 0.9, f2, 1000)\n",
        "\t\tassert np.allclose(x_star, array((1.0, 1.0)))"
      ],
      "metadata": {
        "id": "vj174SJgDFgC"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9zmsQh2DNHE",
        "outputId": "113dce95-c73d-4198-c034-906b020d555f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "BFGS Line Search on test function\n",
            "alpha_0 = 1.000000, x_0 = [ 45.28889838 -92.6107201 ]\n",
            "iteration\talpha_k\t\tx_k\t\t\t\tf_evals\tyk*sk positive?\n",
            "1\t\t1.000000\t[0. 0.]\t5\tyes\n",
            "\n",
            "\n",
            "BFGS Line Search on a second test function\n",
            "alpha_0 = 1.000000, x_0 = [-84.72719696  -5.89662547]\n",
            "iteration\talpha_k\t\tx_k\t\t\t\tf_evals\tyk*sk positive?\n",
            "1\t\t1.000000\t[1. 1.]\t5\tyes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "."
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Newton's Method Line Search on test function\n",
            "alpha_0 = 1.000000, x_0 = [-46.75507361 -46.4323189 ]\n",
            "iteration alpha_k   x_k       f evals   \n",
            "         1    1.0000[0. 0.]         4\n",
            "\n",
            "\n",
            "Newton's Method Line Search on a second test function\n",
            "alpha_0 = 1.000000, x_0 = [-23.50542976  83.876756  ]\n",
            "iteration alpha_k   x_k       f evals   \n",
            "         1    1.0000[1. 1.]         4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "..........."
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "\n",
            "--- Debugging testProblem4_1Hessian ---\n",
            "Input x: [0. 0.]\n",
            "Calculated Hessian: [[42. -0.]\n",
            " [-0. 20.]]\n",
            "Expected Hessian: [[42  0]\n",
            " [ 0 20]]\n",
            "Input x: [1. 0.]\n",
            "Calculated Hessian: [[ 42. -40.]\n",
            " [-40.  20.]]\n",
            "Expected Hessian: [[ 42 -40]\n",
            " [-40  20]]\n",
            "Input x: [0. 1.]\n",
            "Calculated Hessian: [[ 2. -0.]\n",
            " [-0. 20.]]\n",
            "Expected Hessian: [[ 2  0]\n",
            " [ 0 20]]\n",
            "Input x: [1. 1.]\n",
            "Calculated Hessian: [[  2. -40.]\n",
            " [-40.  20.]]\n",
            "Expected Hessian: [[  2 -40]\n",
            " [-40  20]]\n",
            "\n",
            "\n",
            "Steepest Descent Line Search on test function\n",
            "alpha_0 = 1.000000, x_0 = [-59.58863676 -90.36670183]\n",
            "iteration\talpha_k\t\tx_k\t\t\t\tf evals\n",
            "1\t\t0.500000\t[0. 0.]\t4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "..\n",
            "----------------------------------------------------------------------\n",
            "Ran 14 tests in 0.143s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tolerance met: 1, trust region boundary: 9, negative curvature: 0, maximum iterations: 0\n",
            "Number of function evaluations: 32\n",
            "tolerance met: 1, trust region boundary: 12, negative curvature: 0, maximum iterations: 1\n",
            "Number of function evaluations: 44\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7f36b4ed2240>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conjugate Gradient"
      ],
      "metadata": {
        "id": "IoVxWtXXOxFF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hilbert Matrix Code"
      ],
      "metadata": {
        "id": "yENohW_jmUrb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def MakeHilbertMatrix(n):\n",
        "        \"\"\"\n",
        "        Makes a Hilbert matrix of dimension nxn.  The $ij$th component\n",
        "        of the Hilbert matrix is defined to be:\n",
        "\n",
        "        $A_{i,j} = 1/(i+j-1)$\n",
        "\n",
        "        Inputs:\n",
        "                n is the dimension of the desired Hilbert matrix\n",
        "\n",
        "        Outputs:\n",
        "                Returns a Hilbert matrix of dimension nxn\n",
        "\n",
        "        Side Effects:\n",
        "                None\n",
        "        \"\"\"\n",
        "        # Use broadcasting to create the matrix more efficiently\n",
        "        i = np.arange(1, n + 1).reshape(-1, 1) # Column vector from 1 to n\n",
        "        j = np.arange(1, n + 1)               # Row vector from 1 to n\n",
        "        # The formula is 1 / (i + j - 1). Broadcasting handles the element-wise creation.\n",
        "        hilbert_matrix = 1.0 / (i + j - 1)\n",
        "\n",
        "        return hilbert_matrix"
      ],
      "metadata": {
        "id": "JMT0a2MtO09U"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conjugate Gradient Algorithm"
      ],
      "metadata": {
        "id": "lTLGwdQLmPdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ConjugateGradient(x_0, A, b, residual, maxiter):\n",
        "\t\"\"\"\n",
        "\tThis is an implementation of Algorithm 5.2 defined\n",
        "\ton page 111 of Nocedal and Wright.  It provides an\n",
        "\testimate for the inverse of a matrix A to approximately\n",
        "\tsolve the linear system\n",
        "\n",
        "\t$Ax = b$\n",
        "\n",
        "\tWith A an nxn array, x a vector of dimension n, and\n",
        "\tb a solution vector of dimension n.\n",
        "\n",
        "\tInputs:\n",
        "\t\tx_0, an inital estimate of the solution $x^\\ast$\n",
        "\n",
        "\t\tA the nxn matrix for which we need an inverse to\n",
        "\t\t\tsolve the linear system\n",
        "\n",
        "\t\tb the n dimension vector\n",
        "\n",
        "\t\tresidual is the tolerance allowed on $r_k^Tr_k$\n",
        "\n",
        "\t\tmaxiter is the maximum number of iterations allowable\n",
        "\n",
        "\tOutputs:\n",
        "\t\tReturns the number of iterations required to reduce\n",
        "\t\tthe error factor $r_k^Tr_k$ below the residual argument\n",
        "\n",
        "\t\tAlso the value $x^\\ast$ that it found\n",
        "\t\"\"\"\n",
        "\t# $r_0 = r_k \\leftarrow Ax_0 - b$\n",
        "\tr_k = matrixmultiply(A, x_0) - b\n",
        "\n",
        "\t# $p_0 = p_k \\leftarrow -r_0$\n",
        "\tp_k = -r_k\n",
        "\n",
        "\t# $k \\leftarrow 0$\n",
        "\tk = 0\n",
        "\tx_k = x_0\n",
        "\n",
        "\twhile math.sqrt(dot(r_k, r_k)) > residual and k < maxiter:\n",
        "\t\t# $\\alpha_k \\leftarrow \\frac{r_k^Tr_k}{p_k^TAp_k}$\n",
        "\t\talpha_k = dot(r_k, r_k) / MatQuad(p_k, A, p_k)\n",
        "\n",
        "\t\t# $x_{k+1} \\leftarrow x_k + \\alpha_kp_k$\n",
        "\t\tx_k = x_k + alpha_k*p_k\n",
        "\n",
        "\t\t# $r_{k+1} \\leftarrow r_k + \\alpha_kAp_k$\n",
        "\t\tr_k_plus_1 = r_k + alpha_k*matrixmultiply(A, p_k)\n",
        "\n",
        "\t\t# $\\beta_{k+1} \\leftarrow \\frac{r_{k+1}^Tr_{k+1}}{r_k^Tr_k}$\n",
        "\t\tbeta_k_plus_1 = dot(r_k_plus_1, r_k_plus_1) / dot(r_k, r_k)\n",
        "\n",
        "\t\t# $p_{k+1} \\leftarrow -r_{k+1} + \\beta_{k+1}p_k$\n",
        "\t\tp_k = -1.0*r_k_plus_1 + beta_k_plus_1*p_k\n",
        "\n",
        "\t\t# $k \\leftarrow k+1$\n",
        "\t\tk += 1\n",
        "\n",
        "\t\t# advance $r_k$\n",
        "\t\tr_k = r_k_plus_1\n",
        "\n",
        "\treturn (k, x_k)\n"
      ],
      "metadata": {
        "id": "Cfdw0sBPYNb_"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GetCGIterations(n) for Hilbert Matrix"
      ],
      "metadata": {
        "id": "X3yP9OhLmGvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def GetCGIterations(n):\n",
        "\t\"\"\"\n",
        "\tReturns the number of iterations required to solve the\n",
        "\tlinear system $Ax = b$ of dimension $n$ using the\n",
        "\tConjugateGradient algorithm defined above, and with\n",
        "\t$A$ as an nxn Hilbert matrix.\n",
        "\n",
        "\tInputs:\n",
        "\t\tn, the dimension of the system\n",
        "\n",
        "\tOutputs:\n",
        "\t\tThe number of iterations CG required to solve the\n",
        "\t\tsystem\n",
        "\n",
        "\tSide Effects:\n",
        "\t\tNone\n",
        "\t\"\"\"\n",
        "\tA = MakeHilbertMatrix(n)\n",
        "\tb = np.ones(n)\n",
        "\tx_0 = np.zeros(n)\n",
        "\titers, x_star = ConjugateGradient(x_0, A, b, 10e-6, 10000)\n",
        "\treturn iters"
      ],
      "metadata": {
        "id": "jL1--eHzP0YK"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit testing for Conjugate Gradient Code"
      ],
      "metadata": {
        "id": "Z6gOAOYSl9PU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#####################\n",
        "# unit testing code #\n",
        "#####################\n",
        "\n",
        "class ConjugateGradientTestCase(unittest.TestCase):\n",
        "\tdef testConjugateGradient(self):\n",
        "\t\tprint('testing ConjugateGradient')\n",
        "\n",
        "\t\t# CG should solve identity system in one iteration\n",
        "\t\tA = identity(300)\n",
        "\t\tx_0 = np.zeros(300)\n",
        "\t\tb = np.ones(300)\n",
        "\n",
        "\t\t(iters, x_star) = ConjugateGradient(x_0, A, b, 0, 300)\n",
        "\t\tassert iters == 1\n",
        "\n",
        "\t\tremains = matrixmultiply(A, x_star) - b\n",
        "\t\tassert dot(remains, remains) == 0"
      ],
      "metadata": {
        "id": "m5n4wWyoQL0B"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWHo9HU2R0Rf",
        "outputId": "edc89846-9e45-4151-dbd7-15f94742085b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "BFGS Line Search on test function\n",
            "alpha_0 = 1.000000, x_0 = [-43.7544647  -47.04638537]\n",
            "iteration\talpha_k\t\tx_k\t\t\t\tf_evals\tyk*sk positive?\n",
            "1\t\t1.000000\t[0. 0.]\t5\tyes\n",
            "\n",
            "\n",
            "BFGS Line Search on a second test function\n",
            "alpha_0 = 1.000000, x_0 = [75.07556577 83.26035177]\n",
            "iteration\talpha_k\t\tx_k\t\t\t\tf_evals\tyk*sk positive?\n",
            "1\t\t1.000000\t[1. 1.]\t5\tyes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".."
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "testing ConjugateGradient\n",
            "\n",
            "\n",
            "Newton's Method Line Search on test function\n",
            "alpha_0 = 1.000000, x_0 = [60.74464904 98.26163572]\n",
            "iteration alpha_k   x_k       f evals   \n",
            "         1    1.0000[0. 0.]         4\n",
            "\n",
            "\n",
            "Newton's Method Line Search on a second test function\n",
            "alpha_0 = 1.000000, x_0 = [39.70364182 74.51731126]\n",
            "iteration alpha_k   x_k       f evals   \n",
            "         1    1.0000[1. 1.]         4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "..........."
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "\n",
            "--- Debugging testProblem4_1Hessian ---\n",
            "Input x: [0. 0.]\n",
            "Calculated Hessian: [[42. -0.]\n",
            " [-0. 20.]]\n",
            "Expected Hessian: [[42  0]\n",
            " [ 0 20]]\n",
            "Input x: [1. 0.]\n",
            "Calculated Hessian: [[ 42. -40.]\n",
            " [-40.  20.]]\n",
            "Expected Hessian: [[ 42 -40]\n",
            " [-40  20]]\n",
            "Input x: [0. 1.]\n",
            "Calculated Hessian: [[ 2. -0.]\n",
            " [-0. 20.]]\n",
            "Expected Hessian: [[ 2  0]\n",
            " [ 0 20]]\n",
            "Input x: [1. 1.]\n",
            "Calculated Hessian: [[  2. -40.]\n",
            " [-40.  20.]]\n",
            "Expected Hessian: [[  2 -40]\n",
            " [-40  20]]\n",
            "\n",
            "\n",
            "Steepest Descent Line Search on test function\n",
            "alpha_0 = 1.000000, x_0 = [-18.83171102 -31.50826866]\n",
            "iteration\talpha_k\t\tx_k\t\t\t\tf evals\n",
            "1\t\t0.500000\t[0. 0.]\t4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "."
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tolerance met: 1, trust region boundary: 10, negative curvature: 0, maximum iterations: 0\n",
            "Number of function evaluations: 35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 15 tests in 0.130s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tolerance met: 1, trust region boundary: 16, negative curvature: 0, maximum iterations: 0\n",
            "Number of function evaluations: 53\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7f36b4d934a0>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problems in the book"
      ],
      "metadata": {
        "id": "LXfDugDSRO4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def problem_3_1():\n",
        "\tx_0 = array((1.2, 1.2))\n",
        "\talpha_0 = 1\n",
        "\n",
        "\t# does not converge quickly, even after 5500 iterations\n",
        "\t# it only has $x^\\ast$ calculated to 4 decimal places\n",
        "\tx_star = SteepestDescentLineSearch(x_0, alpha_0, \\\n",
        "\t\t\tBacktrackingAlpha, \\\n",
        "\t\t\tRosenbrock(), \\\n",
        "\t\t\t9.9e-2)\n",
        "\n",
        "\tx_0 = array((1.2, 1.2))\n",
        "\talpha_0 = 1\n",
        "\n",
        "\tx_star = NewtonsMethodLineSearch(x_0, alpha_0,\\\n",
        "\t\t\t   BacktrackingAlpha, \\\n",
        "\t\t\t   Rosenbrock(), \\\n",
        "\t\t\t   9.9e-13)\n",
        "\n",
        "\tx_0 = array((-1.2, 1))\n",
        "\talpha_0 = 1\n",
        "\n",
        "\t# again, doesn't converge in a tractable amount of time\n",
        "\tx_star = SteepestDescentLineSearch(x_0, alpha_0, \\\n",
        "\t\t\tBacktrackingAlpha,\\\n",
        "\t\t\tRosenbrock(), \\\n",
        "\t\t\t9.9e-2)\n",
        "\n",
        "\tx_0 = array((-1.2, 1))\n",
        "\talpha_0 = 1\n",
        "\n",
        "\tx_star = NewtonsMethodLineSearch(x_0, alpha_0,\\\n",
        "\t\t\t   BacktrackingAlpha, \\\n",
        "\t\t\t   Rosenbrock(), \\\n",
        "\t\t\t   9.9e-13)\n",
        "\n",
        "def problem_3_9():\n",
        "\tx_0 = array((1.2, 1.2))\n",
        "\talpha_0 = 1\n",
        "\tr = Rosenbrock()\n",
        "\n",
        "\tx_star = BFGSLineSearch(x_0, alpha_0, \\\n",
        "\t\t\tStrongWolfe, \\\n",
        "\t\t\tr, \\\n",
        "\t\t\t9.9e-13, \\\n",
        "\t\t\tlinalg.inv(r.hessian_f(x_0)))\n",
        "\n",
        "\tx_0 = array((-1.2, 1))\n",
        "\n",
        "\tx_star = BFGSLineSearch(x_0, alpha_0, \\\n",
        "\t\t\t\tStrongWolfe,\n",
        "\t\t\t\tr,\n",
        "\t\t\t\t9.9e-13, \\\n",
        "\t\t\t\tlinalg.inv(r.hessian_f(x_0)))\n",
        "\n",
        "def problem_4_1():\n",
        "\t# we need the solution of $p_k$ as a function of $\\Delta$\n",
        "\tprint ('x = (0, -1)')\n",
        "\tprint ('Delta\\tp_k')\n",
        "\n",
        "\tp = Problem4_1(2)\n",
        "\tB_k = p.hessian_f(array((0, -1)))\n",
        "\tepsilon = 9.9e-13\n",
        "\tg = p.grad_f(array((0, -1)))\n",
        "\tfor Delta in arange(0, 2, 0.01):\n",
        "\t\tp_k, t, b, n, m = CG_Steihaug(p, epsilon, Delta, B_k, g)\n",
        "\t\tprint ('%f\\t%s' % (Delta, p_k))\n",
        "\n",
        "\tprint ('x = (0, 0.5)')\n",
        "\tprint ('Delta\\tp_k')\n",
        "\n",
        "\tp = Problem4_1(2)\n",
        "\tB_k = p.hessian_f(array((0, 0.5)))\n",
        "\tepsilon = 9.9e-13\n",
        "\tg = p.grad_f(array((0, 0.5)))\n",
        "\tfor Delta in arange(0, 2, 0.01):\n",
        "\t\tp_k, t, b, n, m = CG_Steihaug(p, epsilon, Delta, B_k, g)\n",
        "\t\tprint ('%f\\t%s' % (Delta, p_k))\n",
        "\n",
        "\n",
        "\n",
        "def problem_4_3():\n",
        "\tmaxiter = 10000\n",
        "\n",
        "\tprint ('***** Trust Region ***** for n = 10:')\n",
        "\tfor i in range(0, 1):\n",
        "\t\tx_0 = []\n",
        "\t\tfor j in range(0, 10):\n",
        "\t\t\tx_0.append(uniform.rvs(loc=-100.0, scale=200.0, size=1)[0])\n",
        "\t\tx_0 = transpose(array((x_0)))\n",
        "\n",
        "\t\tprint ('x_0: %s' % x_0)\n",
        "\n",
        "\t\tx_star = TrustRegion(x_0, 10.0, 1.0, 0.1, Problem4_3(10), 1000)\n",
        "\n",
        "\t\tprint ('x_star: %s' % x_star)\n",
        "\n",
        "\tprint ('***** Trust Region ***** for n = 50:')\n",
        "\tfor i in range(0, 1):\n",
        "\t\tx_0 = []\n",
        "\t\tfor j in range(0, 50):\n",
        "\t\t\tx_0.append(stats.rand.uniform(0, 2)[0])\n",
        "\t\tx_0 = transpose(array((x_0)))\n",
        "\n",
        "\t\tprint ('x_0: %s' % x_0)\n",
        "\n",
        "\t\tx_star = TrustRegion(x_0, 10.0, 1.0, 0.1, Problem4_3(50), 1000)\n",
        "\n",
        "\t\tprint ('x_star: %s' % x_star)\n",
        "\n",
        "\n",
        "def problem_5_1():\n",
        "\tprint ('n\\titerations')\n",
        "\tprint ('5\\t%s' % GetCGIterations(5))\n",
        "\tprint ('8\\t%s' % GetCGIterations(8))\n",
        "\tprint ('12\\t%s' % GetCGIterations(12))\n",
        "\tprint ('20\\t%s' % GetCGIterations(20))\n",
        "\tprint ('100\\t%s' % GetCGIterations(100))"
      ],
      "metadata": {
        "id": "Ac1MZAcMifTJ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "problem_3_1()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WViLuH2-iqWb",
        "outputId": "ad6ed5ef-bf6a-4aa9-96e9-9ed19771548f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Steepest Descent Line Search on the Rosenbrock function\n",
            "alpha_0 = 1.000000, x_0 = [1.2 1.2]\n",
            "iteration\talpha_k\t\tx_k\t\t\t\tf evals\n",
            "1\t\t0.000977\t[1.08710938 1.246875  ]\t13\n",
            "2\t\t0.000977\t[1.11457059 1.23416637]\t25\n",
            "3\t\t0.000977\t[1.11081971 1.23574864]\t37\n",
            "4\t\t0.000977\t[1.11139655 1.23539157]\t49\n",
            "\n",
            "\n",
            "Newton's Method Line Search on the Rosenbrock function\n",
            "alpha_0 = 1.000000, x_0 = [1.2 1.2]\n",
            "iteration alpha_k   x_k       f evals   \n",
            "         1    1.0000[1.19591837 1.43020408]         4\n",
            "         2    0.5000[1.09828449 1.19668813]         8\n",
            "         3    1.0000[1.06448816 1.13199285]        11\n",
            "         4    1.0000[1.01199212 1.02137221]        14\n",
            "         5    1.0000[1.00426109 1.00848056]        17\n",
            "         6    1.0000[1.00005033 1.00008294]        20\n",
            "         7    1.0000[1.00000018 1.00000035]        23\n",
            "         8    1.0000[1. 1.]        26\n",
            "\n",
            "\n",
            "Steepest Descent Line Search on the Rosenbrock function\n",
            "alpha_0 = 1.000000, x_0 = [-1.2  1. ]\n",
            "iteration\talpha_k\t\tx_k\t\t\t\tf evals\n",
            "1\t\t0.000977\t[-0.98945313  1.0859375 ]\t13\n",
            "2\t\t0.001953\t[-1.06433209  1.04417187]\t24\n",
            "3\t\t0.000977\t[-1.02345146  1.0614826 ]\t36\n",
            "4\t\t0.001953\t[-1.0267651   1.05600225]\t47\n",
            "5\t\t0.001953\t[-1.02025638  1.05531644]\t58\n",
            "6\t\t0.001953\t[-1.02383734  1.04969403]\t69\n",
            "7\t\t0.001953\t[-1.01709245  1.04912719]\t80\n",
            "8\t\t0.001953\t[-1.02085423  1.04340448]\t91\n",
            "9\t\t0.001953\t[-1.01396606  1.04291185]\t102\n",
            "10\t\t0.001953\t[-1.01781085  1.03713659]\t113\n",
            "11\t\t0.001953\t[-1.01088111  1.03666875]\t124\n",
            "12\t\t0.001953\t[-1.01470505  1.03089214]\t135\n",
            "13\t\t0.001953\t[-1.00783856  1.03039768]\t146\n",
            "14\t\t0.001953\t[-1.01153765  1.02467146]\t157\n",
            "15\t\t0.001953\t[-1.00483627  1.02409996]\t168\n",
            "16\t\t0.001953\t[-1.00831245  1.01847339]\t179\n",
            "17\t\t0.001953\t[-1.00186919  1.01777831]\t190\n",
            "18\t\t0.001953\t[-1.00503586  1.01229533]\t201\n",
            "19\t\t0.001953\t[-0.99892972  1.01143663]\t212\n",
            "20\t\t0.001953\t[-1.00171634  1.00613349]\t223\n",
            "21\t\t0.001953\t[-0.99600846  1.00507963]\t234\n",
            "22\t\t0.001953\t[-0.99836367  0.99998324]\t245\n",
            "23\t\t0.001953\t[-0.99309498  0.99871244]\t256\n",
            "24\t\t0.001953\t[-0.9949881   0.99383948]\t267\n",
            "25\t\t0.003906\t[-0.98536932  0.99084091]\t277\n",
            "26\t\t0.001953\t[-0.99292431  0.98307208]\t288\n",
            "27\t\t0.001953\t[-0.98294679  0.98417622]\t299\n",
            "28\t\t0.001953\t[-0.98901732  0.97714816]\t310\n",
            "29\t\t0.001953\t[-0.98046957  0.97754156]\t321\n",
            "30\t\t0.001953\t[-0.9851585   0.97120524]\t332\n",
            "31\t\t0.500000\t[0.86838966 0.90440871]\t335\n",
            "32\t\t0.001953\t[0.9708772  0.84569461]\t346\n",
            "33\t\t0.001953\t[0.8974865  0.88354927]\t357\n",
            "34\t\t0.001953\t[0.95262468 0.85305425]\t368\n",
            "35\t\t0.001953\t[0.91229377 0.87431969]\t379\n",
            "36\t\t0.001953\t[0.94259936 0.85789791]\t390\n",
            "37\t\t0.001953\t[0.92029277 0.86984933]\t401\n",
            "38\t\t0.001953\t[0.93707632 0.8608999 ]\t412\n",
            "39\t\t0.001953\t[0.92472127 0.86762339]\t423\n",
            "40\t\t0.001953\t[0.9340559  0.86273513]\t434\n",
            "41\t\t0.001953\t[0.92721664 0.86653407]\t445\n",
            "42\t\t0.001953\t[0.93242923 0.8638765 ]\t456\n",
            "43\t\t0.001953\t[0.92865185 0.8660436 ]\t467\n",
            "44\t\t0.001953\t[0.93157818 0.86461807]\t478\n",
            "45\t\t0.001953\t[0.92950207 0.86587582]\t489\n",
            "46\t\t0.001953\t[0.93115843 0.86513296]\t500\n",
            "47\t\t0.001953\t[0.93002838 0.86588416]\t511\n",
            "48\t\t0.001953\t[0.93097843 0.86552034]\t522\n",
            "49\t\t0.001953\t[0.93037489 0.86598929]\t533\n",
            "\n",
            "\n",
            "Newton's Method Line Search on the Rosenbrock function\n",
            "alpha_0 = 1.000000, x_0 = [-1.2  1. ]\n",
            "iteration alpha_k   x_k       f evals   \n",
            "         1    1.0000[-1.1752809   1.38067416]         4\n",
            "         2    0.1250[-0.93298143  0.81121066]        10\n",
            "         3    1.0000[-0.78254008  0.58973638]        13\n",
            "         4    1.0000[-0.45999712  0.10756339]        16\n",
            "         5    1.0000[-0.39304563  0.15000237]        19\n",
            "         6    0.2500[-0.20941191  0.00677013]        24\n",
            "         7    1.0000[-0.06571902 -0.01632866]        27\n",
            "         8    1.0000[ 0.14204255 -0.02298878]        30\n",
            "         9    1.0000[0.2311072  0.04547802]        33\n",
            "        10    0.5000[0.37974282 0.1181458 ]        37\n",
            "        11    1.0000[0.47959489 0.22004082]        40\n",
            "        12    1.0000[0.65340583 0.39672894]        43\n",
            "        13    1.0000[0.70262363 0.49125758]        46\n",
            "        14    0.5000[0.80278553 0.63322101]        50\n",
            "        15    1.0000[0.86349081 0.74193125]        53\n",
            "        16    1.0000[0.94207869 0.8813362 ]        56\n",
            "        17    1.0000[0.96799182 0.93633667]        59\n",
            "        18    1.0000[0.99621031 0.9916387 ]        62\n",
            "        19    1.0000[0.99947938 0.99894834]        65\n",
            "        20    1.0000[0.99999889 0.99999751]        68\n",
            "        21    1.0000[1. 1.]        71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# problem_3_9()"
      ],
      "metadata": {
        "id": "7qmobhW5iqA9"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "problem_4_1()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "hdZgijraippG",
        "outputId": "8e8bfa60-983a-4c4c-dde6-0e8e7ef9c767"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = (0, -1)\n",
            "Delta\tp_k\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'NaN' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1600292240.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mproblem_4_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-501464182.py\u001b[0m in \u001b[0;36mproblem_4_1\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mDelta\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mp_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCG_Steihaug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'%f\\t%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1825674335.py\u001b[0m in \u001b[0;36mCG_Steihaug\u001b[0;34m(fobj, epsilon, Delta, B_k, g)\u001b[0m\n\u001b[1;32m     69\u001b[0m                         \u001b[0;31m# reached trust region boundary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                         \u001b[0mtm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTauMinimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_j\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_j\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDelta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                         \u001b[0mtau_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbisection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200.0\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mDelta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m                         \u001b[0mtau_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbisection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m200.0\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mDelta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mtau_1\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1902976910.py\u001b[0m in \u001b[0;36mbisection\u001b[0;34m(fobj, a, b, xtol, maxiter)\u001b[0m\n\u001b[1;32m     30\u001b[0m                         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_mid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                         \u001b[0;32mreturn\u001b[0m \u001b[0mNaN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                 \u001b[0mmaxiter\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_x_mid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mxtol\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmaxiter\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'NaN' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "problem_4_3()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "CMsDCrcbipJs",
        "outputId": "6efff95e-00d8-497e-b543-857cb0b5e929"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** Trust Region ***** for n = 10:\n",
            "x_0: [ 73.35215072 -68.78325659 -95.13783805  16.12521385  17.25572796\n",
            " -68.57577558 -56.22698958  23.42134627  32.42071576 -71.5333623 ]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3560622378.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mproblem_4_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-501464182.py\u001b[0m in \u001b[0;36mproblem_4_3\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'x_0: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mx_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mx_star\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrustRegion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mProblem4_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'x_star: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mx_star\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2800652159.py\u001b[0m in \u001b[0;36mTrustRegion\u001b[0;34m(x_0, Delta_Bar, Delta_0, eta, fobj, maxiter)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxiter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0mp_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCG_Steihaug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9.9e-13\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDelta_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhessian_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0;31m# update the exit condition counters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1825674335.py\u001b[0m in \u001b[0;36mCG_Steihaug\u001b[0;34m(fobj, epsilon, Delta, B_k, g)\u001b[0m\n\u001b[1;32m     55\u001b[0m                         \u001b[0mp_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp_j\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtau_1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0md_j\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                         \u001b[0mp_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp_j\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtau_2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0md_j\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp_1\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mp_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "problem_5_1()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHz7TMv6ioSz",
        "outputId": "be55e413-4b79-46ae-e3b8-8d9e21b25031"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n\titerations\n",
            "5\t6\n",
            "8\t19\n",
            "12\t20\n",
            "20\t34\n",
            "100\t112\n"
          ]
        }
      ]
    }
  ]
}